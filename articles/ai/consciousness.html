<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="Alex Khouderchah">

    <title>Consciousness: Lifting the Veil</title>

    <!-- Bootstrap Core CSS -->
    <link href="../../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="../../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Saira" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Dancing+Script" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- Theme CSS -->
    <!-- TODO - REPLACE WITH MIN! -->
    <link href="../../css/agency.css" rel="stylesheet">
    <link href="../../css/general.css" rel="stylesheet">

    <!-- MathJax -->
    <script type="text/javascript" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>

    <!-- code-prettify -->
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
    <style>
      li.L0, li.L1, li.L2, li.L3,
      li.L5, li.L6, li.L7, li.L8 {
          list-style-type: decimal !important;
      }
    </style>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js" integrity="sha384-0s5Pv64cNZJieYFkXYOTId2HMA2Lfb6q2nAcx2n0RTLUnCAoTTsS0nKEO27XyKcY" crossorigin="anonymous"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js" integrity="sha384-ZoaMbDF+4LeFxg6WdScQ9nnR1QC2MIRxA1O9KWEXQwns1G8UNyIEZIQidzb0T1fo" crossorigin="anonymous"></script>
        <![endif]-->

         </head>

  <body id="page-top" class="index bg-medium-gray bg-paper">

    <!-- Navigation -->
    <nav id="mainNav" class="navbar navbar-default navbar-fixed-top page-scroll top-only-show">
      <div class="container">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
            <span class="sr-only">Toggle navigation</span> Menu <i class="fa fa-bars"></i>
          </button>
          <a class="top-unhidden" href="../../index.html">Alex Khouderchah</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav navbar-right">
            <li class="hidden">
              <a href="#page-top"></a>
            </li>
            <li>
              <a class="page-scroll" href="../../articles.html">Articles</a>
            </li>
          </ul>
        </div>
        <!-- /.navbar-collapse -->
      </div>
      <!-- /.container-fluid -->
    </nav>

    <header style="background-image:url('../../img/header-bg.jpg');">
      <div class="container">
        <div class="intro-text">
          <div class="intro-lead-in">Consciousness: Lifting the Veil</div>
      </div>
    </header>

    <!-- Portfolio Grid Section -->
    <section id="portfolio">
      <div class="container">
        <div class="row">
          <p class="text-article" style="font-size: 13px">
            Initially Published: &ensp;2020-12-14<br>
            Last Modified: &ensp;&emsp;&emsp;&thinsp;2022-05-02
          </p>
          <center>
          <button type="button" class="text-article text-button night-toggle" style="text-align: center">
              <span class="sr-only">Toggle navigation</span> Toggle Night Mode&nbsp;
              <i class="fa fa-moon-o"></i>
          </button>
          </center>
          <div class="col-lg-12 bg-light-gray" id="rightRef">
            <h3 class="section-heading text-center">A Lens of a Lens</h3>
            <p class="text-article">
              Seemingly ineffable&#8213;and yet a fundamental part of our
              existence&#8213;consciousness remains a mystery despite progress
              in neuroscience, cognitive science, and AI. The text below
              provides a unified lens with which to view consciousness. This
              lens is not necessarily complete, or an implementation of
              consciousness in human brains, but it is hoped that the lens will
              nonetheless be useful in reasoning about consciousness in
              biological and technological systems alike. Indeed, consciousness
              can itself be seen as a lens. Much like one might reason about a
              higher-dimensional space by looking at multiple
              lower-dimensional <a href="https://en.wikipedia.org/wiki/Projection_(linear_algebra)">projections</a>,
              such concepts as consciousness, intelligence, and
              self-modification serve as ways of viewing specific aspects of
              artificial and biological cognitive systems in an attempt to gain a
              holistic understanding.
            </p>
            <p class="text-article text-no-bottom">
              This article presents consciousness by refining an initial model.
              Here's how the article is layed out:
            </p>
            <ul class="text-article">
              <li>Introducing a single-focused, logical mechanism of control
              called the Conscious System</li>
              <li>Examining the notions of meaning and modality to expand what
              the Conscious System can model</li>
              <li>Removing the requirement of single focus, and viewing human
              consciousness as an interrelation between a multi-focused
              "somatic" Consious System and a single-focused "logical" Conscious
              System</li>
              <li>Considering the different kinds of configurations of Conscious
              Systems that might be possible or expected in different cognitive
              systems</li>
            </ul>
            <p class="text-article text-no-bottom">
              We refine the model of the Conscious System in two major sections:
            </p>
            <ul class="text-article">
              <li>Consciousness-as-control, which considers how consciousness
              can direct the flow of thoughts and influence the mind to some
              degree</li>
              <li>Consciousness-as-experience, which considers how consciousness
              can produce subjective experience</li>
            </ul>

            <h3 class="section-heading text-center">Consciousness-as-control</h3>
            <h4 class="text-center" style="text-transform: none">Metarepresentation</h4>
            <p class="text-article">
              Thoughts, associations, reflexes, etc in biological brains are
              represented as patterns
              of <a href="https://en.wikipedia.org/wiki/Action_potential">neural
              activation</a>, with
              the <a href="https://en.wikipedia.org/wiki/Nervous_system">nervous
              system's</a> architecture providing the "substrate" upon which
              these representations can
              exist. However, <a href="https://en.wikipedia.org/wiki/Metacognition">metacognition</a>
              does not necessarily involve analysis of and interaction with
              direct representations of thought. For example, one may
              consciously perceive a thought as being in English, but the direct
              representation of that thought&#8213;encoded as a specific pattern
              of neural activation&#8213;is not necessarily a neural encoding of
              English. The refutation
              of <a href="https://en.wikipedia.org/wiki/Linguistic_determinism">linguistic
              determinism</a> alone provides enough reason to suspect that
              thought doesn't use language as a base representation, or at least
              not exclusively.
            </p>
            <p class="text-article">
              Instead, we consider metacognition as interacting with
              metarepresentations of thought, natural language being an example in humans. In the context of
              metacognition, metarepresentations enable a system to reason about
              a thought without needing to understand exactly how it was
              originally represented or interpreted.  Indeed, if metacognition
              involved base representations rather than metarepresentations, the
              field of neuroscience would be based far more on introspection
              than it actually is.  We will have much more to say
              on metarepresentation in the sections below, and why introspection
              of one's own conscious experience is insufficient to make general
              claims on the types of metarepresentation employed by all humans.
            </p>

            <div class="article-example">
              <h5 class="text-center" style="text-transform: none">Metarepresentation in Computers</h5>
              <p class="text-article"></p>
              <p class="text-article">
                Before going further, let's talk about metarepresentation in
                computers to make the concept more concrete. Fundamentally,
                everything happening in
                a <a href="https://en.wikipedia.org/wiki/Central_processing_unit">processor</a>
                boils down to patterns of
                <a href="https://en.wikipedia.org/wiki/Bit">bits</a> and
                modifications to patterns of bits. Some representations in the
                form of patterns of bits
                are <a href="https://en.wikipedia.org/wiki/Two%27s_complement#Converting_to_two's_complement_representation">two's
                complement
                signed integers</a>, <a href="https://en.wikipedia.org/wiki/IEEE_754">IEEE
                754 floating point numbers</a>,
                and <a href="https://en.wikipedia.org/wiki/Machine_code">machine
                code</a>. In the words of the brilliant
                textbook <a href="https://en.wikipedia.org/wiki/Structure_and_Interpretation_of_Computer_Programs">SICP</a>,
                programming languages are structured around primitives, means of
                combination, and means of abstraction, for both data and
                code. However, simply combining and abstracting primitive
                representations does not necessarily create a metarepresentation
                (particularly one with causality, which we will discuss below);
                a metarepresentation must represent a representation.
              </p>
              <p class="text-article">
                In the case
                of <a href="https://en.wikipedia.org/wiki/C_(programming_language)">C</a>
                code <a href="https://en.wikipedia.org/wiki/Compiler">compiled</a>
                into
                an <a href="https://en.wikipedia.org/wiki/Executable">executable</a>,
                an example of a metarepresentation is
                the <a href="https://en.wikipedia.org/wiki/Abstract_syntax_tree">abstract
                syntax tree</a> of the C code. The AST is a metarepresentation
                because it is a representation of the machine code contained in
                the executable, which is the base representation. Being a
                metarepresentation doesn't mean that the nature of
                representation is necessarily different than that of the base
                representation; the AST is still simply a pattern of
                bits&#8213;just as metarepresentations in the brain are still
                simply patterns of neural activation&#8213;but the level of
                representation is clearly different. Generally, a base
                representation in a processor is specially handled by a
                particular subsystem, like
                an <a href="https://en.wikipedia.org/wiki/Arithmetic_logic_unit">arithmetic
                logic unit</a>
                or <a href="https://en.wikipedia.org/wiki/Floating-point_unit">floating
                point unit</a>. Metarepresentations, on the other hand, are
                handled by systems built <i>on top</i> of the processor. The C
                compiler, which corresponds to a collection of machine code, is
                what handles the AST.
              </p>
              <img class="img-main" src="../../img/articles/consciousness/ast_metarep.png" alt="AST metarepresentation">
              <p class="text-caption">Figure 1: C code and AST as metarepresentations of procedures.</p>
              <p class="text-article"></p>
              <p class="text-article">
                Now, since we've established that an AST is a form of
                metarepresentation, let us note that an AST corresponds to the
                <a href="https://en.wikipedia.org/wiki/High-level_programming_language">high-level
                code</a> itself. The process
                of <a href="https://en.wikipedia.org/wiki/Lexical_analysis">lexing</a>
                and <a href="https://en.wikipedia.org/wiki/Parsing">parsing</a>
                converts high-level code in the form of text into an AST. In
                this sense, the high-level code is a representation of the AST
                and thus we conclude that while machine code is a base
                representation of a procedure,
                high-level code is (transitively) a
                metarepresentation of a procedure; it just happens that ASTs are
                a much more convenient metarepresentation for compilers
                and <a href="https://en.wikipedia.org/wiki/Interpreter_(computing)">interpreters</a>
                to use. As an aside,
                the <a href="https://en.wikipedia.org/wiki/Homoiconicity">homoiconic</a>
                nature
                of <a href="https://en.wikipedia.org/wiki/Lisp_(programming_language)">Lisp</a>
                is exploited by the language design so that Lisp code is a
                direct textual representation of its AST. This essentially
                removes the parsing step from the conversion between code and
                AST, and is a great convenience in the context of
                <a href="https://en.wikipedia.org/wiki/Metaprogramming">metaprogramming</a>
                and self-modifying systems, but we will save further discussion
                for a future article on self-modification.
              </p>
              <p class="text-article">
                As a final note, we ask the question: since a metarepresentation
                of something is inherently a direct representation of that
                thing, when do we consider something a base representation vs a
                metarepresentation? While a high-level programming language
                could be seen as a metarepresentation of procedures, someone
                learning the language for the first time will learn about it
                specifically as a direct representation of
                procedures. Ultimately, nothing is a metarepresentation in
                general; rather, it can only be a metarepresentation with
                respect to a particular base representation (e.g. in a
                processor, brain, etc). Most modern programming consists of
                considering a high-level language as a direct representation of
                procedures, but someone optimizing
                a <a href="https://en.wikipedia.org/wiki/Hot_spot_(computer_programming)">hot
                spot</a> for a particular processor may indeed think about the
                high-level code as a metarepresentation with respect to the
                processor, trying to tease the compiler into generating the
                desired machine code.
            </p>
            </div>

            <p class="text-article"></p>
            <p class="text-article">
              Finally, we consider the notion of <keyword>causality</keyword>
              for metarepresentations, in which changes to the
              metarepresentation cause a change to the base representation, and
              consequently to the thing being represented. In the case of an AST
              of C code, the compilation step produces the causality of the
              metarepresentation
              (where <a href="https://en.wikipedia.org/wiki/Decompiler">decompilation</a>
              does the reverse, producing a metarepresentation from the base
              representation). As an example of metarepresentation without
              causality, consider the case of the knee-jerk reflex that one
              might have tested at the doctor's. Referring to this in one's head
              as the
              "<a href="https://en.wikipedia.org/wiki/Patellar_reflex">patellar
              reflex</a>" is an example of creating a metarepresentation of that
              reflex in English, but there is no causality of changes to the
              metarepresentation. Referring to the "patellar reflex" instead as
              "blushing" will not cause
              a <a href="https://en.wikipedia.org/wiki/Stretch_receptor">stretching</a>
              in
              the <a href="https://en.wikipedia.org/wiki/Quadriceps_femoris_muscle">quadriceps</a>
              muscle to make you blush; the reflex is represented by the neural
              architecture of
              the <a href="https://en.wikipedia.org/wiki/Reflex_arc">reflex
              arc</a>, of which the English metarepresentation does not have
              causality. Similarly, abstraction in programming languages (e.g.
              the ability to refer to a combination of functional primitives as
              "compute_average" and interact directly with that) could be
              thought of as a simple metarepresentation,
              but <a href="https://en.wikipedia.org/wiki/Name_binding">name
              binding</a> is not causal; changing "compute_average" to
              "compute_median" will not cause the function to compute the median
              rather than average. When a metarepresentation lacks causality, it
              can only correspond to the base representation by being
              specifically constructed to do so. Like changing high-level code
              without re{compil,interpret}ing, changing a metarepresentation
              which lacks causality will cause the metarepresentation and base
              representation to diverge in meaning.
            </p>

            <br>
            <h4 class="text-center" style="text-transform: none">The Conscious System</h4>
            <p class="text-article">
              We begin by presenting the <keyword>Conscious System</keyword> as
              a single-focused, logical system which analyzes and guides a
              causal metarepresentation of thought.
            </p>
            <p class="text-article">
              Within the context of the Conscious System, we can provide a clear
              distinction between the concepts of conscious, subconscious, and
              unconscious thought. Unconscious thought is that for which the
              Conscious System simply lacks a causal metarepresentation, like
              the patellar reflex, functions of
              the <a href="https://en.wikipedia.org/wiki/Autonomic_nervous_system">autonomic
              nervous system</a>, and even functions of relatively higher brain
              structures like
              the <a href="https://en.wikipedia.org/wiki/Limbic_system">limbic
              system</a>. Subconscious thought is that which can have a causal
              metarepresentation, but which either doesn't currently have a
              constructed causal mapping between base representation and
              metarepresentation, or whose causal mapping is not currently used
              by the Conscious System. Finally, conscious thought is
              subconscious thought which the Conscious System is currently
              analyzing and guiding using a causal metarepresentation.
            </p>

            <img class="img-main" src="../../img/articles/consciousness/process_1.png" width="65%" alt="The logical process: approx 1">
            <p class="text-caption">Figure 2: First approximation of the conscious process.</p>
            <p class="text-article"></p>

            <p class="text-article">
              Now, we note that the nervous system is inherently parallel, with
              many distinct (although not necessarily disjoint or isolated)
              patterns of activation occurring at the same time. While pondering
              the nature of consciousness,
              our <a href="https://en.wikipedia.org/wiki/Central_nervous_system">central</a>
              and autonomic nervous systems control such various behaviors
              as <a href="https://en.wikipedia.org/wiki/Vasoconstriction">vasoconstriction</a>, <a href="https://en.wikipedia.org/wiki/Control_of_ventilation">respiration</a>,
              and <a href="https://en.wikipedia.org/wiki/Heart_rate#Influences_from_the_central_nervous_system">heart
              control</a>, various sensory input is relayed by
              the <a href="https://en.wikipedia.org/wiki/Thalamus">thalamus</a>
              to corresponding regions
              of <a href="https://en.wikipedia.org/wiki/Cerebral_cortex">cerebral
              cortex</a>, and a myriad of other singular and repetitive
              activation patterns occur, despite the empirical and introspective
              evidence pointing to the fact that conscious focus
              in <a href="https://en.wikipedia.org/wiki/Dual_consciousness">nonanomolous
              brains</a> is exclusive (i.e. that we can't multi-task).
            </p>
            <p class="text-article">
              Given the multitude of subconscious thoughts that could be made
              conscious and the assumption that conscious focus is exclusive, it
              is interesting to consider conscious attention and its influencing
              factors. Here we define <keyword>attention</keyword> as the valuation of the
              current stream of conscious thought over the multitude of other
              available subconscious thoughts. There is very clearly an ability
              of nonconscious thought in influencing attention. While pondering
              the nature of consciousness, if a lion were to appear,
              the previous conscious thought would quickly be
              preempted, bringing the immediate threat of the lion to the
              Conscious System. This preemption is not conscious, the previous
              conscious thought being about a subject completely distinct from
              the possibility of a lion jumping out. On the other hand,
              practices like meditation and focused studying are examples in
              which attention is consciously modulated.
            </p>

            <img class="img-main" src="../../img/articles/consciousness/process_2.png" width="65%" alt="The logical process: approx 2">
            <p class="text-caption">Figure 3: Second approximation of the conscious process; added attention.</p>
            <p class="text-article"></p>

            <p class="text-article">
              More broadly, we can consider in general how conscious and
              nonconscious thoughts can influence each other. While trying to
              prove a mathematical theorem, one might consciously note that
              certain classes of approaches will not work and may consciously
              enumerate desirable properties of better approaches, but the jump
              to a new approach is not necessarily conscious. In general, it
              seems like such phenomena as "eureka moments" and logical leaps
              are characterized in part by a conscious discontinuity, where the
              previous conscious thought influenced subconscious thought until a
              more promising line of thought took its place. On the other hand,
              nonconscious sensory and emotional processing can certainly
              influence subconscious thoughts, in addition to modulating
              conscious attention. Perhaps unsurprisingly, this
              suggests
              a <a href="https://en.wikipedia.org/wiki/Heterarchy">heterarchical</a>
              relationship between the conscious system and nonconscious
              systems.
            </p>

            <img class="img-main" src="../../img/articles/consciousness/process_3.png" width="65%" alt="The logical process: approx 3">
            <p class="text-caption">Figure 4: Third approximation of the conscious process; added subconscious system.</p>
            <p class="text-article"></p>

            <br>
            <h4 class="text-center" style="text-transform: none">Modalities of Human Metarepresentation</h4>

            <p class="text-article text-no-bottom">
              In this section we will examine the nature of human
              metarepresentations. At any point in time, our conscious
              experience can be characterized by the modality of conscious
              focus. Russell Hurlburt has called the focus of consciousness the
              "Pristine Inner Experience", and distinguishes between five
              frequently occurring kinds<a href="#r2"
              id="rr2"><sup>[2]</sup></a>:
              <ul class="text-article">
                <li>inner speech</li>
                <li>inner sight</li>
                <li>feelings</li>
                <li>sensory awareness</li>
                <li>unsymbolized thought</li>
              </ul>
            </p>
            <p class="text-article">
              We use Hurlburt's categorization, not as an advocation of a
              necessary truth, but simply to apply our lens to it and see what
              insights we can draw. Hurlburt's research found that each of the 5
              modalities of consciousness above served as the dominant aspect of
              conscious focus in roughly 20% of test subjects (even if the
              percentages may vary in
              different <a href="https://en.wikipedia.org/wiki/Sample_(statistics)">samples</a>,
              the variation seen even in a single sample is
              interesting). Furthermore, some people simply do not experience
              one or more of the above modalities of consciousness, at least
              with any significant frequency; to my surprise, some hearing
              people do not regularly experience inner speech. For this reason,
              introspection, while capable of disproving overly restrictive
              statements on the nature of human metarepresentations, is
              insufficient to make or prove such statements. This variation in
              modality of conscious focus between people is perhaps another
              reason why consciousness is so murky a concept.
            </p>
            <div class="article-example">
              <h5 class="text-center" style="text-transform: none">Inner Speech: Auditory Metarepresentation Modality</h5>
              <p class="text-article">
                While some may not regularly experience inner speech, attempting
                to read a long word&#8213;particularly in alphabetic or syllabic writing
                systems, where symbols correspond to sounds&#8213;will
                likely lead
                to <a href="https://en.wikipedia.org/wiki/Subvocalization">subvocalization</a>.
                Try reading
                "<a href="https://en.wiktionary.org/wiki/hippopotomonstrosesquipedaliophobia">hippopotomonstrosesquipedaliophobia</a>".
              </p>
              <p class="text-article">
                A key characteristic of representations based on sound
                is their temporality. When listening to another person
                speak, the ordering of information is imposed by the
                speaker, with the nature of sound demanding an
                ordering of some sort. It's notable that temporality
                seems to be a key element of
                language&#8213;irrespective of the demands of sound as
                a modality&#8213;although the degree to which
                temporality plays a role in the structure of sentences
                (i.e. the <a href="https://en.wikipedia.org/wiki/Syntax">syntax</a>
                of a language) can vary. For example, Latin
                employs <a href="https://en.wikipedia.org/wiki/Latin_declension">declensions</a>
                to explicitly encode such information as grammatical
                case, whereas English implicitly encodes this
                information through word ordering. Because of
                this, <a href="https://en.wikipedia.org/wiki/Latin_word_order">Latin
                word ordering</a> enjoys a relative freedom, serving
                more to indicate connotation than denotation. However,
                this freedom is at the level of sentences; if this
                article were written in Latin, the sentences and
                paragraphs would still need to ordered appropriately
                to be comprehendible (relatively speaking).
              </p>
            </div>
            <p class="text-article"></p>
            <div class="article-example">
              <h5 class="text-center" style="text-transform: none">Inner Sight: Visual Metarepresentation Modality</h5>
              <p class="text-article">
                The visual modality differs from auditory in that ordering is
                not necessarily imposed by the representation. Looking at Figure
                4 above, there is not a fixed ordering in which the viewer must
                analyze the content which the diagram represents (although
                fields like advertising and graphic design may exploit
                heuristics to statistically influence how humans perceive visual
                representations). Indeed,
                while <a href="https://en.wikipedia.org/wiki/Computer_vision">computer
                vision</a> models generally analyze entire images as an atomic
                unit, "image processing" in humans is centered
                around <a href="https://en.wikipedia.org/wiki/Fixation_(visual)">fixations</a>
                and <a href="https://en.wikipedia.org/wiki/Saccade">saccades</a>,
                where the small visual region corresponding to
                the <a href="https://en.wikipedia.org/wiki/Fovea_centralis">fovea</a>
                is most deeply analyzed.
              </p>
              <p class="text-article">
                That the visual modality does not require ordering is not
                sufficient to claim that visual metarepresentations necessarily
                exploit this freedom from temporality. While many readers may
                experience visual thought in one form or another, tickertaping
                refers specifically to seeing
                the <a href="https://en.wikipedia.org/wiki/Orthography">orthographic</a>
                appearance of words one thinks<a href="#r3"
                id="rr3"><sup>[3]</sup></a>. This is a stark example of the
                distinction between modality and language of metarepresentation,
                where a single language may have representations in multiple
                modalities. Taking this further, someone who thinks to
                themselves but sees a series of non-orthographic images is not
                necessarily thinking in a manner completely detached from
                natural language.
              </p>
              <p class="text-article">
                As an aside, it would be interesting to see if the prevalence of
                tickertaping varies amongst, say, native Italian and Mandarin
                speakers. While written Chinese is not entirely semantic, the
                majority of characters
                being <a href="https://en.wiktionary.org/wiki/phonosemantic">phonosemantic</a>,
                it certainly contains more semantic information than written
                Italian
                (a <a href="https://en.wikipedia.org/wiki/Orthographic_depth">shallow
                orthography</a>). To date, I've been unable to find information
                on this.
              </p>
            </div>
            <p class="text-article"></p>
            <div class="article-example">
              <h5 class="text-center" style="text-transform: none">Sensory Awareness: Not a Metarepresentation</h5>
              <p class="text-article">
                Both inner speech and inner sight correspond
                to <a href="https://en.wikipedia.org/wiki/Stimulus_modality">sensory
                modalities</a>, but the origin of the content is the mind
                itself; inner speech and inner sight are metarepresentations of
                thought. Sensory awareness, as we define it here, is
                distinguished by being a direct representation of sensory
                information (of any sensory modality). The content comes
                from <em>outside</em>. <a href="https://en.wikipedia.org/wiki/Communication">Communication</a>
                is an exception, where I would argue that consciously focusing
                on the content of the communication does not qualify as sensory
                awareness (as opposed to, say, focusing on the sound of a
                person's voice as they speak), but we will defer communication
                for now.
              </p>
              <p class="text-article">
                In this sense, the degree to which one's conscious focus resides
                in sensory awareness seems to be partially a matter of conscious
                attention. One could consciously savor every bite of a meal, or
                scarf it down in seconds while thinking about something
                else. Importantly, sensory awareness encompasses not just the
                awareness of particular sensory information, but also how the
                sensory information "feels". We must defer further discussion of
                this to the section on consciousness-as-experience below.
              </p>
            </div>
            <p class="text-article"></p>
            <div class="article-example">
              <h5 class="text-center" style="text-transform: none">Structural or "Unsymbolized" Thought: Metarepresentation of Non-Sensory Modality</h5>
              <p class="text-article">
                The defining trait of "unsymbolized", or structural, thought is
                that its metarepresentation doesn't correspond to a sensory
                modality; as before, we note that language and modality are not
                necessarily coupled. Lingering on the notion of language, it's
                worth highlighting the fact that language as a tool for
                communication relies on shared context (what we often refer to
                as "culture" or "common sense"). If one traveled back 1000 years
                in time and wanted to complain about their commute, so much
                about the modern world would need to be explained before
                something as simple as "my commute sucks" would be even remotely
                comprehendible, taking for granted that we can even assume a
                shared language.
              </p>
              <p class="text-article">
                The notion of context is still relevant for language as a tool
                for metarepresentation rather than for communication with
                others. Experiencing inner speech does not imply that
                communicating one's thoughts to another would be trivial; there
                may be context in the mind beyond common sense and culture that
                would need to be shared before the original thought could be
                communicated. One of the beauties of natural language is that it
                enables us as humans not just to communicate immediate thoughts,
                but also to make explicit the context within which those
                thoughts are embedded. Importantly, while language is inherently
                temporal and serial, the contexts it can represent encompass a much
                broader set of structures, whether this be a description in
                English of a system with interrelated parts, or an
                implementation of circular
                <a href="https://en.wikipedia.org/wiki/Linked_list">linked
                lists</a> in C
                (compare <a href="https://en.wikipedia.org/wiki/Anaphora_(linguistics)">anaphora</a>
                and <a href="https://en.wikipedia.org/wiki/Cataphora">cataphora</a>
                in natural language
                to <a href="https://en.wikipedia.org/wiki/Reference_(computer_science)">references</a>
                in programming languages). Thus we could assume that at least
                some of the context of a thought is represented as structural
                thought despite the fact that the individual may experience
                something like inner speech. While often experiencing structural
                thought alongside other modalities, I personally find that
                trying to understand what another is saying precludes inner
                speech, and thus makes it more likely that my conscious focus
                will be primarily structural.
              </p>
              <p class="text-article">
                In the context of AI, it seems important to note the distinction
                between "unsymbolized" and "unsymbolizable". As an example, Lisp
                is
                a <a href="https://en.wikipedia.org/wiki/S-expression">textual
                representation of binary trees</a> with leaves populated by data;
                <a href="https://en.wikipedia.org/wiki/Cons">cons cells</a> can
                also construct circular structures that cannot generally be
                represented as Lisp text, but dialects like Common Lisp
                provide <a href="http://clhs.lisp.se/Body/02_dho.htm">means of
                textually representing</a> certain circular structures. Another
                example
                are <a href="https://en.wikipedia.org/wiki/Ontology_(information_science)">ontology</a>
                languages
                like <a href="https://en.wikipedia.org/wiki/Web_Ontology_Language">OWL</a>,
                which are textual representations of potentially very complex
                knowledge graphs. Ultimately, ontology languages can be thought
                of as a means
                of <a href="https://en.wikipedia.org/wiki/Serialization">serializing</a>
                the in-memory ontology in a human-{read,writ}able form. In both
                of these examples, the in-memory representation is only
                partially symbolized, but can be textually represented
                completely symbolically; partly for this reason, I prefer to
                refer to this modality as "structural" rather than
                "unsymbolized".
                Even <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">artificial
                neural networks</a> (ANNs or just NNs) are symbolizable; consider
                some <a href="https://en.wikipedia.org/wiki/TensorFlow">TensorFlow
                1</a> code symbolically representing a computational graph
                corresponding to the forward pass of the NN, along with
                parameters populated from training. It remains an open question
                as to whether NNs can be meaningfully symbolized&#8213;what
                could be referred to as making the model "interpretable",
                allowing developers to interpret the semantics of the resulting
                system&#8213;in general. It is my opinion that finding a general
                method of meaningfully symbolizing NNs is not necessary to
                usefully employ them or any
                other <a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning">probably
                approximately correct</a> model without resorting to end-to-end
                neural systems, which spread the characteristic semantic opacity
                and inability to explicitly correct behavior to the entire
                system.
              </p>
            </div>
            <p class="text-article"></p>

            <p class="text-article">
              For fun, we consider the case of "inner speech" for
              those with pre-lingual deafness. Certainly inner sight remains a
              modality of metarepresentation, but some people seem to think in
              sign language in a manner that is
              more <a href="https://en.wikipedia.org/wiki/Proprioception">proprioceptive</a>
              than visual. Lacking fluency in sign language, an example I have
              personal experience with is the common phenomenon amongst rock
              climbers of "visualizing" oneself climbing a particular route,
              allowing the climber to strategize an appropriate route without
              wearing out the body. In my case, the modality is distinctly
              proprioceptive rather than visual. In either of
              these cases, the experience seems to lie outside of Hurlburt's 5
              categories, not qualifying as sensory awareness due to the
              nature of the content.
            </p>
            <p class="text-article">
              Discussing different modalities naturally leads to the question:
              what <em>are</em> these modalities and why is there such
              individual variance? It is notable that subvocalization during
              reading has been associated with aiding in reading comprehension;
              two classes of hypotheses are that
              the <a href="https://en.wikipedia.org/wiki/Phonology">phonological</a>
              pathways in the brain facilitate the accessing of word meanings,
              or the retaining of those word meanings in short-term memory
              for semantic integration<a href="#r4"
              id="rr4"><sup>[4]</sup></a>. Viewed through this lens, we might
              see different modalities of metarepresentation as the product of
              the Conscious System leveraging different subconscious brain
              regions to aid in metacognition; inner speech could serve the same
              purpose as subvocalization, but here the pathway is used for the
              content of a subconscious thought rather than the content of text
              on a page. Indeed, we have been considering the Conscious System
              to be a distinct system, but much of it can simply be a part of
              the subconscious system that is specialized for metacognition.
              Leveraging other subconscious regions to partake in
              metarepresentational mappings, the "Conscious System" can extend
              itself into subconscious regions not specialized for
              metacognition. The exact means by which this would occur seems to
              rely on the state of the subconscious system in a way that leaves
              much room for nature and nurture to create individual variance
              (e.g. genetic predispositions, development
              during <a href="https://en.wikipedia.org/wiki/Critical_period">critical
              periods</a>, personal preference for favoring certain means of
              thought or actions influencing development of brain regions,
              etc). The remainder of the "Conscious System" here would be an
              unconscious system driving conscious attention. Thus the conscious
              system in this model is the result of an unconscious system
              leveraging the subconscious system to leverage itself in order to
              analyze one of its own thoughts.
            </p>

            <img class="img-main" src="../../img/articles/consciousness/process_4.png" width="65%" alt="The logical process: approx 1">
            <p class="text-caption">Figure 5: Fourth approximation of the conscious process; maximally leveraging existing subconscious machinery.</p>
            <p class="text-article"></p>

            <br>
            <h4 class="text-center" style="text-transform: none">Metalinguistic Abstraction</h4>
            <p class="text-article">
              As discussed in SICP, <keyword>metalinguistic
              abstraction</keyword> is the process of solving a class of
              problems by creating a language which better represents the
              problem domain. Metalinguistic abstraction in programming involves
              using a programming language to create a different programming
              language which better abstracts the relevant class of
              problems. One of many examples in the book consists of simulating
              circuits of
              digital <a href="https://en.wikipedia.org/wiki/Logic_gate#Symbols">logic
              gates</a>. The original language
              is <a href="https://en.wikipedia.org/wiki/Scheme_(programming_language)">Scheme</a>
              (a dialect of Lisp), which is based on abstractions like cons
              cells (pairs), numbers,
              symbols, <a href="https://en.wikipedia.org/wiki/Anonymous_function">lambdas</a>
              (unnamed functions),
              and <a href="https://en.wikipedia.org/wiki/Function_application">functional
              application</a>. The created language is based on the abstractions
              of logic gates and wires. Importantly, simulating a particular
              circuit in this new language does <em>not</em> involve specifying
              how computations must be ordered if, for example, one of the
              inputs changed from 0 to 1. This is done effectively by
              exploiting <a href="https://en.wikipedia.org/wiki/Lazy_evaluation">lazy
              evaluation</a>, but Scheme is
              exclusively <a href="https://en.wikipedia.org/wiki/Eager_evaluation">eagerly
              evaluated</a>; making this change necessitates a modification of
              the interpreter. The only way to introduce lazy evaluation in a
              language that is exclusively evaluated eagerly <em>without</em>
              modifying the interpreter is to build an interpreter supporting
              lazy evaluation on top of the base interpreter and use that to
              evaluate code in the new language.
            </p>
            <p class="text-article">
              Indeed, since metalinguistic abstraction involves changing the
              language, it generally requires changing the system which
              evaluates the language (here, compiler or interpreter). In
              traditional programming language systems, there are limitations
              with respect to how interpreters for multiple languages can
              coexist and interact with the base interpreter. 3-LISP is an
              example of a language system in which code can be provided either
              as code being interpreted or as code doing the interpretation
              (recursively, in what can be viewed as an infinite tower of
              reflective interpreters)<a href="#r5" id="rr5"><sup>[5]</sup></a>.
              We'll save this discussion for a future article on interpreters,
              but needless to say, 3-LISP enables metalinguistic abstraction in
              a manner that is much more robust and dynamic than traditional
              languages allow.
            </p>
            <p class="text-article">
              In the context of the brain, we can look at something like
              cortical specialization through the lens of metalinguistic
              abstraction. Different regions of the cortex
              (e.g. <a href="https://en.wikipedia.org/wiki/Visual_cortex">visual
              cortex</a>
              vs <a href="https://en.wikipedia.org/wiki/Auditory_cortex">auditory
              cortex</a>) may "specialize" not just by containing different
              models, but also by representing those models in languages most
              appropriate to the domain. Since metalinguistic abstraction
              involves creating a language by leveraging an appropriate set of
              abstractions, this could be achieved simply by building models on
              top of other models, assuming the abstractions provided by the
              base models yield sufficient expressivity;
              while <a href="https://en.wikipedia.org/wiki/Turing_completeness">Turing
              completeness</a> is not necessarily relevant here, it's worth
              noting that the logic gate language discussed above retains the
              Turing completeness of Scheme, since a Turing complete processor
              can be represented as a circuit made of wires and logic gates. We
              might look at modalities of human metarepresentation as arising
              from the different "languages" used by these cortical regions.
            </p>
            <p class="text-article">
              Metalinguistic abstraction is also relevant in the context
              of <a href="https://en.wikipedia.org/wiki/Deep_learning">deep
              learning</a>. One of the key differences between "plain" machine
              learning with NNs and deep learning is the focus
              on <a href="https://en.wikipedia.org/wiki/Feature_learning">representation
              learning</a>; that is, by having a "deep" model containing many
              layers of neurons, shallow layers can serve to provide a more
              appropriate set of representations for deeper layers to
              use. Indeed, many deep learning architectures have multiple deeper
              subsystems leveraging the same shallower representations. A common
              example of this are embedding layers (e.g. to
              produce <a href="https://en.wikipedia.org/wiki/Word_embedding">word
              embeddings</a> in natural language processing systems). While I
              have serious qualms with the kinds of representations end-to-end
              neural systems use&#8213;particularly in the context
              of <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">artificial
              general intelligence</a>&#8213;the ability of deep learning models
              to perform metalinguistic abstraction is undoubtedly powerful for
              building certain specialized subsystems
              or <a href="https://en.wikipedia.org/wiki/Weak_AI">narrow AI</a>
              systems. On the note of metalinguistic abstraction requiring
              changes to the interpreter, we point out that the base interpreter
              here is fixed (for example, the fixed subsystems of Tensorflow
              which allow
              for <a href="https://www.tensorflow.org/guide/saved_model">SavedModels</a>
              to be executed). Since each layer of an NN corresponds to a
              nonlinear mapping
              (a <a href="https://en.wikipedia.org/wiki/Feedforward_neural_network">feedforward
              neural network</a> corresponding to
              a <a href="https://en.wikipedia.org/wiki/Function_composition">composition</a>
              of nonlinear mappings), we can look at the mapping as being both
              interpretation and modification of the input representation. This
              mixing of roles (among a number of other factors) makes it very
              difficult in general to take a single layer or chain of layers in
              isolation and compare it to something we as humans would consider
              an abstraction. Nevertheless, successful training of a neural
              network should result in a probably approximately correct mapping
              (to some degree) between input and desired output, particularly
              within the context
              of <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">i.i.d. assumption</a>.
            </p>
            <p class="text-article">
              When different parts of a system express information using
              different languages, metarepresentations can also serve as a way
              of bridging information between regions ("regions" being an
              intentionally fuzzy concept here). Natural language in this scenario
              might serve as a common metarepresentation simply because of its
              expressiveness and the existence of specialized cortical regions
              for language processing.
            </p>
            <p class="text-article">
              For fun, we wrap up this section by comparing the temporality of
              natural language to
              the <a href="https://en.wikipedia.org/wiki/Call_stack">call
              stack</a> in computing. Ignoring more complex forms of control
              flow for simplicity, we can view program execution as consisting
              of a top-level function which can optionally call itself or any
              number of other functions, which themselves can also optionally
              call any number of functions (compare to the
              <a href="https://en.wikipedia.org/wiki/Structured_program_theorem">structured
              program theorem</a>). In order to keep track of where to return
              after one function completes, function invocations involve pushing
              a
              new <a href="https://en.wikipedia.org/wiki/Call_stack#Structure">frame</a>
              onto the call stack, which contains the return location in
              addition to other information like the values of function
              arguments and local variables. The structure and information of
              the call stack can be used by tools
              like <a href="https://en.wikipedia.org/wiki/Debugger">debuggers</a>
              so that programmers can examine and
              modify <a href="https://en.wikipedia.org/wiki/Runtime_(program_lifecycle_phase)">runtime</a>
              behavior. While debug builds provide
              additional <a href="https://en.wikipedia.org/wiki/Debug_symbol">debug
              information</a> and
              shirk <a href="https://en.wikipedia.org/wiki/Optimizing_compiler">aggressive
              optimization</a> to make the debugging experience more palatable,
              the structure of the call stack is inherent to how programs
              execute rather than existing specifically for debugging. Many
              systems will produce
              a <a href="https://en.wikipedia.org/wiki/Stack_trace">stack
              trace</a> when a fatal error occurs, so that developers can see
              what chain of function invocations preceded the error. Similarly,
              one could think of ways to structure thought temporally so that
              the base representation inherently corresponds to the structure of
              language, facilitating the mapping between the base representation
              of thought and language as a metarepresentation of the thought.
            </p>

            <br>
            <h4 class="text-center" style="text-transform: none">The Metacognitive Loop</h4>
            <p class="text-article">
              We begin this section by defining
              the <keyword>understanding</keyword> of some information as
              contextually useful access to ways in which that information can
              be used. For example, understanding the statement "we're out of
              milk" enables such behavior as planning to go to the store,
              rethinking cereal for breakfast, or yelling at the last person to
              use the milk. Understanding is <em>not</em> the following of any
              of these paths; understanding is what enables these to be
              potential paths at all. Understanding is "deeper" when it can
              provide more potential paths (more ways to use the information),
              when the paths are more fruitful (more contextually useful), and,
              recursively, when the understanding that results from taking a
              path is "deep" as well. Thus, understanding is a contextual model
              of ways in which information can be used. What we might refer to
              when we talk about "understanding a field" would here be
              considered as a rich, interrelated set of understandings rather
              than a single understanding in isolation. This definition need not
              satisfy every informal usage of the word "understanding", but all
              uses of the word in this section will refer to it specifically
              as defined here. A deeper inspection of
              this topic must be deferred to a future article on intelligence.
            </p>
            <p class="text-article">
              In considering different metarepresentations of thought,
              understanding seems to be a metarepresentation particularly
              well-suited to the process of guiding thought. In essence, guiding
              a thought consists of choosing a particular "path of thought"
              provided by its understanding, and causality of the
              metarepresentation consists of actually leading the thought down
              that path; in this sense, understanding implicitly rather than
              explicitly represents the original thought. What we call
              the <keyword>metacognitive loop</keyword> is the loop by which
              a <i>subconscious</i> thought undergoes a series of
              metarepresentational mappings to create/update understanding, the
              Conscious System <i>subconsciously</i> determines which path to
              take, and a new <i>subconscious</i> thought is created with which
              the metacognitive loop can start over. What we perceive as the
              conscious thought are metarepresentations of the subconscious
              thought (whether an intermediate metarepresentation like inner
              speech, or the resulting understanding); more on this in the next
              section. This loop also allows for understanding to be built up
              incrementally, rather than requiring each immediate thought to be
              analyzed in isolation.
            </p>
            <p class="text-article">
              From our conscious perspective, this might be perceived as the
              simultaneous creation and analysis/control of conscious thought
              (i.e. simultaneous cognition and metacognition). Even in an
              inherently parallel system like the brain, these two steps can't
              actually happen simultaneously, if for nothing else but inherent
              communication delays. Without necessarily making a claim on the
              meaning of
              macroscopic <a href="https://en.wikipedia.org/wiki/Neural_oscillation">brain
              waves</a>, we note that the range of frequencies&#8213;from the
              slow delta waves (~1-4Hz) to the rapid gamma waves
              (~30-70Hz)&#8213;leave plenty of room for non-simultaneous
              processes to occur within a single oscillatory period. Considering
              a computer simulation of a physical process running on
              a <a href="https://en.wikipedia.org/wiki/Single-core">single-core
              processor</a>, while the objects (atoms, molecules, macroscopic
              objects, etc) have simultaneous interactions on the scale of a
              single simulation timestep, the interactions are not simultaneous
              on the scale of a
              single <a href="https://en.wikipedia.org/wiki/Instruction_cycle">instruction
              cycle</a>, ignoring limited exceptions
              like <a href="https://en.wikipedia.org/wiki/SIMD">SIMD</a>
              instructions. Thus "simultaneous" processes can still be
              implemented serially, but the speed of the simulation timesteps
              may be affected. Somewhat tangentially, the highly parallel nature
              of the brain imposes serious constraints on the ability to
              perform <a href="https://en.wikipedia.org/wiki/Real-time_simulation">real-time
              simulations</a> of brain-like systems. Artificial NN architectures
              seek to minimize or completely avoid cyclic connections for the
              performance of training and runtime
              &#8213;with <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">RNN</a>
              architectures constraining recurrence to a small part of the total
              system (consider how RNNs are represented as
              single <a href="https://keras.io/guides/working_with_rnns/">layers
              in Keras</a>)&#8213;while the brain has a truly astounding amount
              of cyclic connections at many levels, which exploits the brain's
              parallelism. This doesn't necessarily mean that computer
              architectures must radically change in order to achieve AGI. After
              all, computer science is fundamentally about deeply understanding
              problems and how they relate to the nature of computers, rather
              than trying to blindly copy the natural world. In the space of
              computing, nature is an inspiration, not an instruction manual.
            </p>
            <p class="text-article text-no-bottom">
              One consequence of the metacognitive loop as described above is
              that metacognition (e.g. the decision to take a particular mental
              path over another) occurs subconsciously, rather than
              consciously. How does this fit with the fact that we can perform
              conscious metacognition? As an example, by consciously thinking
              about metacognition in one's own mind (i.e. performing conscious
              meta-metacognition) one could create a list of common
              metacognitive strategies, like:
            </p>
            <ul class="text-article text-no-bottom">
              <li>breaking problems into independent subproblems</li>
              <li>creating hypotheses and verifying that logical
                consequences match reality</li>
              <li>determining which information is
                most relevant to the problem</li>
            </ul>
            <p class="text-article">
              Indeed, fields of study
              like <a href="https://en.wikipedia.org/wiki/Epistemology">epistemology</a>
              and <a href="https://en.wikipedia.org/wiki/Philosophy_of_mind">philosophy
              of mind</a> serve as testaments to the human ability of conscious
              metacognition. The premise here is that conscious metacognition
              can occur, but is based on "historical data" rather than the
              conscious focus being on metacognition as it occurs in real-time;
              that is, only by pausing conscious base cognition can we have
              conscious metacognition. For example, one might recognize a common
              metacognitive strategy
              (e.g. the <a href="https://en.wikipedia.org/wiki/Reductionism">reductionist</a>
              approach of breaking problems into subproblems) by solving a
              problem and then thinking back to see what strategy was
              employed. The ability of conscious thought to traverse different
              levels of metacognition is a matter of attention and
              understanding. Feelings like laziness or frustration can serve as
              an attentional impetus to move from base cognition to
              metacognition, while metacognitive strategies can also be a part
              of understanding and conscious modulation of attention, allowing
              experience to shape the traversal of metacognitive levels in a
              manner more efficient than resorting to something like
              frustration. Early physics students may have it drilled into them
              to begin solving problems by looking for the most relevant
              information, after which the strategy need not be the content of
              conscious focus. Essentially, when understanding is capable of
              representing metacognitive thoughts as opposed to only base
              cognitive thoughts, the metacognitive loop can be applied at
              higher levels of cognition.
            </p>
            <p class="text-article">
              In a sense, this imposes some limitation to the degree of
              conscious control we can impose on our conscious thoughts at any
              one point in time; conscious metacognition allows us to "pick up"
              a thought and place it somewhere else, but switching back to base
              cognition relinquishes conscious metacognitive control. However,
              the ability to consciously train metacognitive strategies makes
              this limitation seem far less significant when looking at longer
              timescales, even on the scale of days or weeks, let alone on the
              scale of a lifetime. One of the characteristics
              of <a href="https://en.wikipedia.org/wiki/Flow_(psychology)">flow</a>
              is focus on a task that is not only uninterrupted by unrelated
              thoughts, but also uninterrupted by switches from conscious base
              to meta cognition; the experience "flows" because we don't
              interrupt thoughts by picking them up and moving them elsewhere.
            </p>
            <p class="text-article">
              As a final speculative note, the subconscious system in the brain
              could be demarcated, from a 10,000 foot view, by
              the <a href="https://en.wikipedia.org/wiki/Neocortex">neocortex</a>,
              with parts of
              the <a href="https://en.wikipedia.org/wiki/Prefrontal_cortex">prefrontal
              cortex</a> serving as specialized metacognitive
              regions. Within this model, one might look at certain neural
              oscillations between the prefrontal cortex and other cortical
              regions as being representative of the ongoing metacognitive loop
              which is so central to consciousness-as-control. Even from this
              highly simplified view, the concept of "finding where
              consciousness lies in the brain" seems a bit silly. The section
              below on consciousness-as-experience will even further muddy
              the waters.
            </p>

            <p class="text-article">
              <b>In conclusion:</b> Consciousness-as-control refers to the
              process by which a Conscious System analyzes and guides a thought
              of the subconscious system. We theorize that this process in the
              brain consists of an unconscious system leveraging the
              subconscious system to analyze and guide one of its own thoughts
              using a metacognitive loop.
            </p>

            <h3 class="section-heading text-center">Consciousness-as-experience</h3>
            <p class="text-article">
              In
              philosophy, <a href="https://en.wikipedia.org/wiki/Qualia">qualia</a>
              refers to subjective conscious sensations, like the fact that pain
              or warmth "feel like something". Chalmers' hard problem of
              consciousness&#8213;which essentially states that mechanisms
              cannot be provided to explain qualia&#8213;is based on the
              premises that qualia exist and
              are <a href="https://en.wikipedia.org/wiki/Irreducibility">irreducible</a>. This
              section describes a theory that agrees with these
              premises&#8213;to at least some degree&#8213;while arriving at
              a very different conclusion, refuting the existence of a "hard
              problem" of consciousness which cannot be described by
              mechanism. This theory can be taken separately from the
              formulation of consciousness-as-control, but is built on top of
              it.
            </p>
            <p class="text-article">
              We begin with the observation that the unconscious
              system contains a robust model of the body, even if this
              model is not entirely explicit or centralized. The fact
              alone that the CNS is capable of integrating sensory
              information from
              the <a href="https://en.wikipedia.org/wiki/Peripheral_nervous_system">PNS</a>
              and providing top-down control is a testament to this
              mental model of the body. Whether it be the thought of
              food triggering salivation, fear triggering
              a <a href="https://en.wikipedia.org/wiki/Fight-or-flight_response">fight-or-flight
              response</a>, or visual exposure to light at night
              triggering a frequency-dependent inhibition
              of <a href="https://en.wikipedia.org/wiki/Melatonin">melatonin</a>
              production, the ability of the CNS in interpreting and
              controlling the state of the body through this model is
              an integral part of our existence. It is also clear that
              this model is primarily unconscious rather than
              subconscious. If we have subconscious mental models of
              such body parts
              as <a href="https://en.wikipedia.org/wiki/Salivary_gland">salivary
              glands</a> or
              the <a href="https://en.wikipedia.org/wiki/Smooth_muscle">smooth
              muscles</a> that enable vasoconstriction, it is only
              because progress in anatomy has made this information
              available (and still we must consciously endeavor to
              understand this information before it is available as
              subconscious mental models); the unconscious models,
              however, are built-in, even if they may change over the
              course of a
              lifetime. The <a href="https://en.wikipedia.org/wiki/Somatotopic_arrangement">somatotopic
              map</a> in
              the <a href="https://en.wikipedia.org/wiki/Primary_somatosensory_cortex">primary
              somatosensory cortex</a> is an example of a built-in
              subconscious model of a part of the body (specifically
              the body surface, with a few exceptions), which we will
              further discuss below.
            </p>
            <p class="text-article">
              There is also some unconscious model of the global "mind" or
              "self" that encompasses much of what we call "emotion". Unlike
              sensory awareness, emotion is not localized to a particular part
              of the body. The
              historical <a href="https://en.wikipedia.org/wiki/Cardiocentric_hypothesis">cardiocentric
              hypothesis</a> shows how our model of "mind" is not localized; if
              it were, humans would not need to consciously hypothesize on its
              anatomical location of reference, oftentimes reaching different
              conclusions. Compare to the example that burning one's hand won't
              result in a conscious deliberation of whether the hand or abdomen
              was the site of pain (ignoring things like remapping of
              somatotopic arrangements, as might happen after losing a limb,
              and <a href="https://en.wikipedia.org/wiki/Referred_pain">referred
              pain</a>). That unconscious emotions can affect subconscious
              thought should be apparent to all human readers
              (sorry <a href="https://en.wikipedia.org/wiki/Language_model">language
              models</a>, you wouldn't get it). While there doesn't seem to be a
              direct means of conscious thought controlling the unconscious
              model of the mind, conscious endeavors like
              <a href="https://en.wikipedia.org/wiki/Cognitive_behavioral_therapy">cognitive
              behavioral therapy</a> and meditation show how conscious thought can indirectly
              influence this model.
            </p>
            <p class="text-article">
              Reiterating our existing mechanistic formulation of consciousness,
              we have a small part of the unconscious system creating
              consciousness-as-control by maximally leveraging the
              subconscious system. The unconscious system here retains some
              control over the metacognitive loop through attention, and by the
              fact that it can influence subconscious thoughts. Being inherently
              parallel, the brain can of course have more than one metacognitive
              loop occurring at once, although the final section below gives
              some reason to suspect that the number of effectively integrated,
              simultaneous metacognitive loops for a particular physical
              architecture would be relatively limited.
            </p>

            <br>
            <h4 class="text-center" style="text-transform: none">The Somatic-Logical Dual Consciousness</h4>
            <p class="text-article">
              Here we theorize that two distinct, simultaneous metacognitive
              loops occur in the brain. Recalling our definition of
              understanding as a contextual model of ways to use information,
              it is notable that understanding need not be as general as we
              often think of it. The metacognitive loop we've described above
              will be referred to as the <keyword>logical
                consciousness</keyword>, and can be characterized as allowing
              for rather general understanding (or at least, the
              inconceivability of that which its understanding cannot
              represent makes logical consciousness seem general with respect
              to itself; it is at least general enough to enable conscious
              metacognition, while the notion of the metacognitive loop only
              guarantees nonconscious metacognition). Note that calling this
              consciousness "logical" does not imply that all thoughts within
              the metacognitive loop would necessarily be considered logical. Examples of
              this are left as an exercise for the reader.
            </p>
            <p class="text-article">
              The second metacognitive loop will be referred to as
              the <keyword>somatic consciousness</keyword> ("somatic" here has
              no relation to anatomical distinctions between the somatic and
              visceral nervous systems). The understanding used in the
              metacognitive loop of somatic consciousness is less general,
              primarily serving as a model of ways to use
              information <em>specifically</em> about the body and its
              surroundings. It seems unlikely that such a form of
              understanding would enable conscious metacognition, but forgoing
              generality provides some noteworthy benefits here. In
              particular, the statement that conscious focus is exclusive
              seems less relevant for the somatic consciousness. By narrowing
              the scope of understanding and the kinds of thoughts analyzed by
              the metacognitive loop, the context of understanding can be
              largely shared by multiple different thoughts. For example,
              while an understanding
              of <a href="https://en.wikipedia.org/wiki/Vector_space">vector
                spaces</a> likely can't be used directly in applying the
              metacognitive loop to a thought
              about <a href="https://www.investopedia.com/articles/retirement/08/convert-401k-roth.asp">Roth
                rollovers</a>, different internal and external sensory
              information could be analyzed with a large shared component of
              understanding, roughly representing the current state of the
              body and its immediate surroundings. Lacking the generality of
              the logical consciousness, investing in a non-exclusive
              attentional mechanism that can leverage shared understanding for
              multiple simultaneous thoughts seems more fruitful for the
              somatic consciousness. In a sense, we might view the somatic
              consciousness as a family of interrelated, simultaneous
              metacognitive loops rather than a single metacognitive loop.
            </p>
            <p class="text-article">
              Recall that the unconscious system heavily leverages the
              subconscious system in the case of logical consciousness. We
              theorize that somatic consciousness still leverages the
              subconscious system for sensory processing, but that the core
              metacognitive loop for somatic consciousness is retained within
              the unconscious system. Going back to our 10,000 foot view of the
              subconscious system being demarcated by the neocortex, we would
              expect to see lots
              of <a href="https://en.wikipedia.org/wiki/Projection_fiber">projection
              fibers</a> between the cortex&#8213;particularly regions which
              process information relevant to modeling the body's internal state
              and its immediate surroundings&#8213;and the lower brain
              structures which it
              surrounds. Indeed, <a href="https://en.wikipedia.org/wiki/Thalamocortical_radiations">thalamocortical
              radiations</a> seem to largely satisfy this requirement (of
              course, non-exclusively; consider pathways from the neocortex
              through
              e.g. the <a href="https://en.wikipedia.org/wiki/Striatum">striatum</a>),
              and also
              include <a href="https://en.wikipedia.org/wiki/Medial_dorsal_nucleus">projections
              to the prefrontal cortex</a>.
            </p>
            <p class="text-article">
              Interestingly, by the somatic consciousness leveraging the
              subconscious system for sensory processing, the subconscious
              system is also capable of forming its own integrated model of the
              body and surrounding world by having the different cortical
              regions communicate with each other. The result is two distinct
              but deeply intertwined models forming alongside one another and
              influencing the other. One could view this as a split between
              unconscious and subconscious components of somatic consciousness,
              although we will defer discussion as to whether these
              constitute distinct consciousnesses to the final section. In this
              sense, metarepresentations kill two birds with one stone.
              Metarepresentations allow the unconscious system to leverage
              subconscious processing for the somatic consciousness, while also
              serving as a means of bidirectional communication between the
              somatic and logical consciousnesses.
              <a href="https://en.wikipedia.org/wiki/Recurrent_thalamo-cortical_resonance">Thalamo-cortical
                resonance</a> does indeed seem to play a key role in both
              perception and consciousness.
            </p>
            <p class="text-article">
              We could look at the metacognitive strategies employed by the
              metacognitive loop of somatic consciousness as being driven by
              emotion rather than more general metacognition. Emotion here
              serves as a mediator between the two otherwise-conflicting factors of
              needing a wide range of behavior to account for the variability
              of the potential environment, while desiring consistent rather
              than erratic behavior at any point in time. Thus emotion
              provides a partial means of controlling a metacognitive loop
              without requiring the metacognitive machinery and generality of
              logical consciousness.
            </p>
            <br>

            <img class="img-main" src="../../img/articles/consciousness/dual.png" width="65%" alt="Somatic-Logical Dual Consciousness">
            <p class="text-caption">
              Figure 6: Approximation of the relationship between somatic and
              logical consciousnesses.
            </p>
            <p class="text-article"></p>

            <p class="text-article">
              Another interesting consequence of having a subconscious
              component of somatic consciousness is that some somatic thoughts
              become available as subconscious thoughts to the logical
              consciousness; sensory awareness consists of logical conscious
              focus being applied to somatic thoughts. While there is clearly
              some ability of logical consciousness in directly modifying the
              model built up by the somatic consciousness&#8213;control of
              <a href="https://en.wikipedia.org/wiki/Skeletal_muscle">voluntary
                muscles</a> quickly coming to mind&#8213;this direct control is
              seemingly limited. It is one thing to visualize a ball falling
              down, and another to see it in reality. The memory of warmth in
              the middle of winter is not nearly as comforting as the actual
              sensation. To a very real degree, the unconscious components of
              both somatic and logical consciousness serve as a means of
              upholding the biological imperatives of survival, reproduction,
              and energy conservation. If the memory of warmth seemed like the
              real thing while stranded in the arctic tundra, or if the threat
              of a black bear could be visualized away into a teddy bear,
              survival of the individual and ultimately the species would
              begin to look dubious (although pretending a black bear is a
              teddy bear is still better than trying to run away from
              it). Similarly, limiting logical access to the somatic model of
              the body confers survival benefits. Giving children the ability
              to modify the workings of their organs through logical
              introspection seems like a good way of preventing survival into
              reproductive age, exerting
              an <a href="https://en.wikipedia.org/wiki/Evolutionary_pressure">evolutionary
                pressure</a> to not have this ability if it were present.  Note
              that this doesn't necessarily place an absolute limit on the
              ability to use logical consciousness in modifying parts of the
              somatic model, but simply that we can't do so directly (more on
              this below). What we call "listening to one's body", here would
              be seen as focusing logical attention on part of the somatic
              consciousness through the subconscious somatic.
            </p>
            <p class="text-article">
              In the previous section, we mentioned that sensory awareness is
              not a metarepresentation of thought. However, since nothing is a
              metarepresentation in general, we must clarify. Sensory awareness
              is not a metarepresentation with respect to the subconscious
              system, but may be with respect to the unconscious system. Taking
              the somatotopic map as an example, this subconscious mapping of
              the body surface is a direct representation within the
              subconscious system, but could be a metarepresentation with
              respect to the unconscious model of the body. In that case it
              could serve as an interface between the two systems, allowing
              communication despite the fact that the unconscious system has a
              much deeper built-in model of the body than the subconscious
              system. Of course, this goes both ways; the subconscious system
              can also have much deeper models than the unconscious system, as
              might be seen between the visual cortex
              and <a href="https://en.wikipedia.org/wiki/Lateral_geniculate_nucleus">lateral
              geniculate nucleus</a>.
            </p>

            <br>
            <h4 class="text-center" style="text-transform: none">Qualia</h4>
            <p class="text-article">
              At this point, we return to the question of what qualia are and
              whether they can be described by mechanism. The most
              staunch <a href="https://en.wikipedia.org/wiki/Mind%E2%80%93body_dualism#Cartesian_dualism">substance
              dualist</a>, for example, could accept the mechanism of
              somatic-logical dual consciousness (I tend to avoid the term
              "dualism" here because that is often used in this context to refer
              specifically
              to <a href="https://en.wikipedia.org/wiki/Mind%E2%80%93body_dualism">mind-body
              dualism</a>) while rejecting the idea that it could explain
              qualia. Returning to the discussion of "unsymbolized" vs
              "unsymbolizable", a substance dualist would consider qualia to be
              unsymbolizable (meaningfully or not), and therefore a
              simulation of neural activations in the brain would not be
              capable of reproducing the subjective experience that occurred
              alongside the pattern of neural
              activation. An <a href="https://en.wikipedia.org/wiki/Interactionism_(philosophy_of_mind)">interactionist</a>
              might argue that somatic consciousness could serve as a physical
              mechanism of interacting with a non-physical substance which
              serves as the true source of qualia, perhaps as a radio picks up
              electromagnetic signals, although here the substance of the
              signals would likely be beyond the realm of physics. Certainly, if
              one finds it inconceivable that a hypothetical, fully-accurate
              digital simulation of a human nervous system&#8213;necessarily
              requiring simulation of bodily systems and the universe around the
              body&#8213;would not be able to experience qualia, this discussion
              will not be convincing. However, the purpose of this discussion is
              not to convince; it simply would be a shame to terminate this
              section without analyzing qualia after having described somatic
              consciousness.
            </p>
            <p class="text-article">
              We view subjective experience from two perspectives that are not
              as mutually exclusive as they might seem, but first, let's
              consider an analogy. In a video game that is
              not <a href="https://en.wikipedia.org/wiki/Procedural_generation">procedurally
              generated</a>, developers manually create and arrange the content
              of the game world, with
              a <a href="https://en.wikipedia.org/wiki/Game_engine">game
              engine</a> allowing developers and players alike to experience and
              interact with the game world through a computer. Taking
              a <a href="https://en.wikipedia.org/wiki/Direct_and_indirect_realism">representationalist</a>
              stance, we could consider the brain as forming a sort of video
              game, but here the game world is formed from sensory input (among
              other things) rather than being prefabricated. Much like a video
              game running with no one playing, simply creating and updating this
              representation of the world doesn't mean much. What we refer to as
              consciousness-as-control is the means by which the mind "plays
              the video game"; we've above described two different conscious
              processes, which interact with somewhat different content and in
              somewhat different ways, although we claim they are both based
              around the idea of metacognitive
              loops. Consciousness-as-experience refers to the fact that
              consciousness-as-control&#8213;that playing the
              game of life-as-experienced-by-you&#8213;can produce subjective experience.
            </p>
            <p class="text-article">
              Continuing with the video game analogy, one could look at being
              immersed in an actual video game as collapsing these two layers of
              video games; being immersed in
              a <a href="https://en.wikipedia.org/wiki/First-person_(video_games)">first-person
              game</a> means playing as the game character, rather than playing
              as yourself playing as the game character. Poor graphics don't
              necessarily stand in the way because consciousness interacts with
              the brain's model of reality, not reality itself, and therefore
              the "game engine" inside the brain can fill in any gaps. Taking
              this further, immersion in a novel is another example of creating
              a different reality, but the means of creating this reality is
              based on the interpretation of text rather than more general
              sensory input. In dreams, not even text is required to form a
              representation of reality; the mind can form a representation of
              reality using nothing but itself (something something politics).
              We might think of empathy as the degree to which one can
              approximately construct the reality of another, with absolute
              close-mindedness or indifference to any reality other than one's
              own as the lower bound. Perhaps one reason
              why <a href="https://en.wikipedia.org/wiki/Mysticism#Mystical_experiences">mystical
              experiences</a> can be so impactful is that they force the person
              to confront their built-in narrow-mindedness to the space of
              conscious experience. While I believe each of us has a unique
              conscious experience that cannot be exactly replicated in
              another's mind&#8213;a belief which the above discussion seems to
              leave room for&#8213;I think one needs to be careful when
              considering the degree of uniqueness. A Bengal cat living amongst
              Siamese cats might find itself to be exceptionally unique, but
              only because it fails to realize that being a cat was not a
              foregone conclusion.
            </p>
            <p class="text-article">
              With a hopefully clearer view of where
              consciousness-as-experience fits into the broader picture of
              the mind, we now consider how this might arise from
              consciousness-as-control.
            </p>

            <div class="article-example">
              <h5 class="text-center" style="text-transform: none">The Representationalist Perspective</h5>
              <p class="text-article">
                This perspective claims that conscious experience arises from a
                conscious process applying attention to a representation of
                itself, in addition to the already-discussed focuses of
                attention like representations of the body and the world around
                the body. Starting with the somatic consciousness, we might look
                at emotion not only as a means of directing the somatic
                metacognitive loop, but also as a partial representation of the
                somatic consciousness's current state itself. In a sense, while
                we think of our body as part of ourselves, from the perspective
                of a conscious process, just dealing with a model of a body need
                not confer a sense of self. Modeling the conscious process
                itself produces
                a <a href="https://en.wikipedia.org/wiki/Self-reference">self-referentiality</a>
                that might enable the development of a deeper sense of
                identity. Having machinery for a conscious process to partly
                focus attention on itself also suggests a built-in bias for
                having a sense of self, for which there would likely be a strong
                evolutionary pressure.
              </p>
              <p class="text-article">
                The <a href="https://en.wikipedia.org/wiki/Attention_schema_theory">attention
                schema theory</a> proposes that conscious experience arises from
                the brain constructing a model of attention. Indeed, attention
                is a part of a conscious process.  It's interesting to consider
                means of superimposing a model of attention on top of existing,
                non-self-referential models; take, as an example, a sort
                of <a href="https://en.wikipedia.org/wiki/Vector_field">vector
                field</a> of attention superimposed on a model of the body
                surface. We might view the logical consciousness as having a
                subconscious model of attention that is partly controlled
                unconsciously (perhaps this model serves as a causal
                metarepresentation of logical attention, allowing for
                unconscious control from afar). However, modeling attention
                alone does not seem sufficient as a representation of a
                conscious process. I would extend the theory by positing that
                conscious processes here model understanding as well as
                attention, emotion being a form of understanding used by the
                somatic consciousness.
              </p>
              <p class="text-article">
                Turning to the logical consciousness, having a
                model of understanding is certainly important for general
                metacognition. In an environment where there is always more
                knowledge to gain and experience to be had, having a rough idea
                of how deeply one understands a situation or concept allows for
                educated guesses about whether one needs to gather more
                information or is ready to act beyond
                information-gathering. Modeling subconscious understanding, however, seems to
                go further than that. Taking sound as an example,
                "understanding" the sound of a saxophone could certainly involve
                relating to one's body of knowledge
                about <a href="https://en.wikipedia.org/wiki/Wind_instrument">wind
                instruments</a>, but it seems likely that the majority of
                information the understanding would relate to is of the auditory
                modality (i.e. represented in a language based on models
                appropriate to the modality). Similarly, seeing a deep red
                cherry, one's understanding of the visual stimulus could relate
                it to natural language labels for&#8213;or a body
                of <a href="https://en.wikipedia.org/wiki/Descriptive_knowledge">descriptive
                knowledge</a> about&#8213;the fruit or its color, but relating
                to other visual stimuli (e.g. relating the color to the deep red
                of blood, pomegranates, or one's favorite shirt) is a much
                richer avenue for relating the information of the stimulus to
                other information. Thus understanding of sensory information
                would certainly relate to logical information, but would likely
                relate much more deeply to other sensory information, and
                vice-versa.
              </p>
              <p class="text-article">
                Returning to the brief discussion on call stacks being a source
                of information about a program's current state that is inherent
                to how the program runs, we might also wonder if subconscious
                understanding can be modeled based on how it inherently
                works. Without claiming to be a complete explanation, we turn to
                the concept
                of <a href="https://en.wikipedia.org/wiki/Spreading_activation">spreading
                activations</a>. In the context of useful association,
                understanding might be seen as a means of modifying how and
                where activations spread, which could be useful for controlling
                metacognitive loops, influencing future thought, etc. In having
                activations spread, making some paths lead to a central location
                or set of locations ("central" might be specific to just a
                particular region, with other central locations for other
                regions) could be used to provide information about the nature
                of the activation spreading characterized by that
                understanding. Something like
                a <a href="https://en.wikipedia.org/wiki/Self-organizing_map">self-organizing
                map</a> could enable the resulting (in
                the <a href="https://en.wikipedia.org/wiki/Limit_(mathematics)">limit</a>) <a href="https://en.wikipedia.org/wiki/Space-filling_curve">space-filling
                curve</a> to support built-in regions&#8213;i.e. built-in models
                of understanding&#8213;while also enabling the creation and
                refinement of new regions, partially with respect to existing
                regions. Importantly, this representation of understanding would
                likely not be causal.
              </p>
              <p class="text-article">
                Briefly, we note that one might consider this as
                an <a href="https://en.wikipedia.org/wiki/Eliminative_materialism#Illusionism">illusionist</a>
                perspective, in that a conscious process creates an illusion of
                subjective experience. However, I prefer to see it as
                representationalist; within the somatic consciousness, anger is
                not an illusion, but a representation of a real state of
                control. We defer interaction between consciousnesses to the
                next perspective.
              </p>
            </div>
            <p class="text-article"></p>
<div class="article-example">
              <h5 class="text-center" style="text-transform: none">The Emergentist Perspective</h5>
              <p class="text-article">
                We have already briefly discussed the concept of different
                regions of the subconscious somatic communicating with each
                other to reach distributed consensus. However, there is no
                reason why the subconscious logical would be isolated from the
                subconscious somatic.  From
                an <a href="https://en.wikipedia.org/wiki/Emergentism">emergentist</a>
                perspective, we might see the distributed consensus amongst both
                somatic and logical subconscious thoughts as producing a form of
                conscious experience beyond just the sum of somatic and logical
                consciousness. Each conscious process receives, beyond the
                "normal" input, information from the other conscious
                process. For example, emotion as a partial representation of the
                somatic consciousness might be used for self-reference within
                the somatic consciousness, but could also be used as input for
                the logical consciousness. In some sense, "self" is the result
                of these two consciousnesses reaching a consensus on what might
                otherwise be mutually exclusive models of self. Indeed, while
                the distinction between somatic and logical consciousnesses may
                be a useful lens, it's very unlikely one could take activity in
                the neocortex and label it as subconscious somatic or
                subconscious logical.
              </p>
              <p class="text-article">
                The concept of distributed consensus between both logical and
                somatic subconscious thoughts also provides some insight into the
                ability of logical consciousness indirectly modifying the
                somatic model. Changing the logical focus changes the set of
                subconscious models which interact to approach consensus. This could
                be in the form of logical attention on metarepresentations
                of logical thought, like inner
                speech, or of logical attention on metarepresentations
                of somatic thought, like sensory awareness or
                feelings. Deep in a state
                of flow,
                the consistent logical attention on a particular task modifies
                the consensus being reached. Similarly, during a meditative
                session where logical attention is fixed on the sensory
                awareness of one's nostrils while breathing, one again changes
                the set of subconscious thoughts which interact, but here the
                logical attention is used to emphasize and deepen an existing
                somatic thought rather than create thoughts which lie outside
                of the somatic model. In either case, we might see this as
                logical attention modulating this emergent conscious experience.
              </p>
              <p class="text-article">
                As a final note, this consensus need not be distributed. For
                example, the prefrontal cortex could control how the
                subconscious somatic and logical thoughts interact. However, as
                we discuss in the final section, there doesn't seem to be a
                strong design pressure for this to occur, and doing so would
                impose its own requirements, so I default to this being
                distributed for now.
              </p>
            </div>
            <p class="text-article"></p>

            <p class="text-article">
              Combining the two perspectives, we conclude by describing
              conscious experience in humans as the product of the conscious
              processes constructing and focusing on models of themselves,
              within a system that is continuously tending towards a consensus
              between somatic thoughts arising from sensory information and
              logical thoughts arising from a general metacognitive loop that
              can be&#8213;but is not necessarily&#8213;consciously directed
              through conscious metacognition.
            </p>
            <p class="text-article">
              We might define <keyword>qualia</keyword> as elements of this
              model of self that consciousness constructs. While not typically
              considered a part of "self", something like the "redness" of a
              cherry falls under the umbrella of qualia as defined here because
              this redness is based on the internal process of visual
              recognition. Assuming this self-representation is not causal with
              respect to the subconscious logical (logical attention perhaps
              being an exception here), qualia <em>truly would</em> be atomic
              and irreducible from the perspective of logical introspection. One cannot
              consciously introspect and "break down" something like fear,
              because to do so requires reducibility with respect to the
              subconscious logical. Much like developing a subconscious model
              for the process of vasoconstriction in a particular artery,
              developing a subconscious model for qualia (like we are currently
              trying to do) does not mean that the model has causality, even if
              it is accurate. Creating labels like "fear", "joy", or "pain" for
              concepts that are not causally represented by those labels is
              reminiscent of name binding in programming languages. Generally,
              lacking an ability to reason about qualia themselves, we resort to
              reasoning about the stimulus and the effects of response. For this reason,
              philosophical approaches to explaining qualia that are based
              squarely on introspective analysis seem a bit frivolous
              to me, taking for granted that all thought can be conscious
              thought without explicitly tackling this assumption (see
              the <a href="https://en.wikipedia.org/wiki/Knowledge_argument#Thought_experiment">Mary's
              room thought experiment</a> for an example of a particularly
              egregious offender).
            </p>
            <p class="text-article">
              Regardless of how one interprets subjective experience, the
              results of it are quite real. It affects how we treat other
              humans, living beings, and the planet. Subjective experience has
              helped enable the greatest of humanity's achievements and the most
              atrocious of cruelties. It affects how we learn, how we form our
              models of the world around us, that remain long after the
              provoking subjective experience has passed. In our daily lives,
              philosophical squabbles over the nature of subjective experience
              seems somewhat less important than how we allow those experiences
              to influence our behavior. However, as we continue to build more
              complex systems in the field of AI, this discussion becomes very
              real and concrete. Dismissal of such discussion as frivolous is
              the dismissal of the consequences of building such systems;
              willful ignorance is best paired with willful quiescence.
            </p>

            <h3 class="section-heading text-center">Is Consciousness Inevitable?</h3>
            <p class="text-article">
              A <a href="https://en.wikipedia.org/wiki/Metasystem_transition">metasystem
              transition</a> occurs when a system develops a higher level of
              organization or control; out of the system comes a (possibly
              implicit) <a href="https://en.wikipedia.org/wiki/Meta-system#In_cybernetics">metasystem</a>.
              The paradigmatic metasystem transition is the transition
              from <a href="https://en.wikipedia.org/wiki/Unicellular_organism">unicellular</a>
              to <a href="https://en.wikipedia.org/wiki/Multicellular_organism">multicellular</a>
              life, where the cell transitions from being the entire system to
              being an instanced subsystem. Metasystem transition theory
              generally considers an explicit metasystem, but metasystem
              transitions seem to often begin without an explicit control. For
              example, one of the simplest known multicellular life consists of
              four mostly-identical cells working together<a href="#r6"
              id="rr6"><sup>[6]</sup></a>; clearly there is no explicit
              controlling structure here.
            </p>

            <br>
            <h4 class="text-center" style="text-transform: none">The Centralization Pressure</h4>
            <p class="text-article">
              It is striking how often
              metasystem transitions eventually result in explicit
              metasystems, despite them not being a foregone conclusion. Looking at the human body, the nervous system truly
              seems to be an explicit controlling structure. Not only does it
              control the body and its actions through direct innervation, but
              the CNS also controls
              the <a href="https://en.wikipedia.org/wiki/Endocrine_system">endocrine
              system</a> through
              <a href="https://en.wikipedia.org/wiki/Neuroendocrine_cell">neuroendrocrine
              cells</a>. The endocrine system&#8213;which leverages
              the <a href="https://en.wikipedia.org/wiki/Circulatory_system">circulatory
              system</a> to
              spread <a href="https://en.wikipedia.org/wiki/Hormone">hormones</a>
              throughout the body&#8213;allows the CNS to influence cells far beyond
              the reach of direct innervation.
            </p>
            <p class="text-article">
              Here we consider whether such centralization of control was
              necessary or
              if <a href="https://en.wikipedia.org/wiki/Decentralised_system">decentralized</a>
              control would work just as well. Having mentioned the nervous
              system, it's worth noting that communication networks and explicit
              metasystems are not necessarily the same thing, although clearly
              control requires the ability to communicate. Evolutionarily, the
              nervous system seems to have developed
              simultaneously&#8213;although at different rates&#8213;as a
              specialized communication network and as an explicit
              metasystem. On the other hand, engineers and computer scientists
              have already developed means of communication for accessing
              physically disparate memory stored anywhere in
              the <a href="https://en.wikipedia.org/wiki/Memory_hierarchy">memory
              hierarchy</a> of a
              machine, <a href="https://en.wikipedia.org/wiki/Inter-process_communication">IPC</a>
              for communication
              between <a href="https://en.wikipedia.org/wiki/Process_(computing)">processes</a>
              within a
              single <a href="https://en.wikipedia.org/wiki/Operating_system">OS</a>
              kernel,
              and <a href="https://en.wikipedia.org/wiki/Computer_network">computer
              networks</a> for communication between processes running in
              potentially distinct operating systems or physical machines. While
              software systems can certainly build novel communication systems
              on top of existing communication mechanisms, the matter of
              developing control
              is <a href="https://en.wikipedia.org/wiki/Abstraction_(computer_science)">abstracted</a>
              from the matter of communication. Even so, design of control
              inherently places demands on the means of communication, and
              therefore this abstraction
              is <a href="https://en.wikipedia.org/wiki/Leaky_abstraction">leaky</a>. Indeed,
              beyond optimizing asymptotic factors for the relevant problem sizes,
              designing <a href="https://en.wikipedia.org/wiki/Cache_(computing)">cache</a>-friendly
              behavior is a major part of making programs fast, since the speed
              of data communication has not increased as rapidly as the speed of
              operating on existing data.
            </p>
            <p class="text-article text-no-bottom">
              The three main design factors for control we consider here
              are:
              <ul class="text-article">
                <li><a href="https://en.wikipedia.org/wiki/Latency_(engineering)">latency</a>
              (delay between stimulus and appropriate response)</li>
                <li>information required to make a decision</li>
                <li>generality of the control system</li>
              </ul>
            </p>
            <p class="text-article">
              Interaction with the world as we generally conceive of it requires
              relatively fast responses. While organisms like plants
              and <a href="https://en.wikipedia.org/wiki/Sponge">sponges</a>
              interact with the world in less latency-critical ways (and indeed,
              they do not have nervous systems like
              all <a href="https://en.wikipedia.org/wiki/Triploblasty">triploblasts</a>
              and
              many <a href="https://en.wikipedia.org/wiki/Diploblasty">diploblasts</a>),
              the animal way of life is generally not very forgiving about high
              latency. Simply taking a step forward, the possibility of
              tripping, of strong wind blowing one back, or of an insufficient
              muscular response following an intense leg workout demand fast
              response to avoid a fall. Reflexes represent the pinnacle of
              latency-sensitivity, triggering responses as fast as the nervous
              system can manage. On the note of reflexes (as opposed to the CNS
              integrating information and triggering actions), it's worth
              looking at control designs through latency relative to the speed
              of the communication mechanism rather than absolute latency. For
              example, most plants certainly have large latency considering
              animal time-scales, but relative to the speed
              of <a href="https://en.wikipedia.org/wiki/Plant_to_plant_communication_via_mycorrhizal_networks">mycorrhizal
              networks</a>, the latency of control may not be as large as it
              initially appears.
            </p>
            <p class="text-article">
              The information required to make a decision also plays a large
              role in control design. Reflexes&#8213;which utilize very limited
              information to trigger a response, even when the CNS can modulate
              responses&#8213;are very easy to control independently of the rest
              of the organism. As information requirements grow for each node
              making a decision, so does the
              total <a href="https://en.wikipedia.org/wiki/Bandwidth_(computing)">bandwidth</a>-latency
              product between control nodes and information sources; that
              is, with an increased volume of data, either the communication
              "pipes" must be enlarged or more time must be allowed for the
              increased volume to be transported through the
              pipes. <a href="https://en.wikipedia.org/wiki/Fat_tree">Fat tree
              networks</a> form a particularly efficient means of increasing the
              total bandwidth capacity of the communication system, but demands
              either centralization or greatly increased latency. The human
              nervous system seems to roughly form a
              <a href="https://en.wikipedia.org/wiki/Branching_factor">wide</a>
              fat tree network
              with <a href="https://en.wikipedia.org/wiki/Nerve_root">nerve
              roots</a>, branches,
              and <a href="https://en.wikipedia.org/wiki/Ganglion">ganglia</a>
              at intermediate nodes and the brain at the root node (although the
              ANS,
              especially <a href="https://en.wikipedia.org/wiki/Enteric_nervous_system">ENS</a>,
              provides many wrinkles to this simplification). Considering that
              this centralization has more to do with the means of connecting
              spatial regions than the control mechanism in and of itself, it's
              worth pointing out the vascular system,
              where <a href="https://en.wikipedia.org/wiki/Artery">arteries</a>
              roughly form one fat tree network
              and <a href="https://en.wikipedia.org/wiki/Vein">veins</a>
              another, with the root nodes connected by
              the <a href="https://en.wikipedia.org/wiki/Pulmonary_circulation">pulmonary
              circulation</a> and corresponding leaf nodes connected
              by <a href="https://en.wikipedia.org/wiki/Capillary">capillaries</a>
              (ignoring exceptions
              like <a href="https://en.wikipedia.org/wiki/Circulatory_anastomosis">arterio-venous
              anastomoses</a>&#8213;some of which
              the <a href="https://en.wikipedia.org/wiki/Hypothalamus">hypothalamus</a>
              can control to aid in temperature
              regulation&#8213;and <a href="https://en.wikipedia.org/wiki/Portal_venous_system">portal
              venous systems</a>). In a sense, as the amount and sources of
              information grow, so does the pressure to centralize control
              unless total bandwidth-latency products can continue increasing
              (likely with
              superlinear <a href="https://en.wikipedia.org/wiki/Asymptotic_analysis">asymptotic</a>
              complexity), disparate information doesn't need to be integrated,
              or generality can be sacrificed.
            </p>
            <p class="text-article text-no-bottom">
              On top of communications demands imposed by fundamental
              information requirements, a control protocol imposes
              additional bandwidth-latency demands. Consider the different
              requirements imposed by:
              <ul class="text-article">
                <li><a href="https://en.wikipedia.org/wiki/Mode_(statistics)">mode</a>
                  with arbitrary tie-breaking</li>
                <li><a href="https://en.wikipedia.org/wiki/Majority">simple
                  majority</a> requiring debate and revote in case
                  of <a href="https://en.wikipedia.org/wiki/Plurality_(voting)">plurality</a></li>
                <li>predefining specific nodes for handling specific contexts,
                  like reflexes, although not necessarily as limited in amount of integrated information</li>
                <li>non-recurrent
                  <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">CNN</a>
                  approximating
                  a <a href="https://en.wikipedia.org/wiki/Q-learning">Q
                  function</a>, often referred to as a deep Q-network or DQN.
                  Here control is partly distributed but strictly hierarchical,
                  with the control protocol moving in a consistent, predefined
                  direction (i.e. forward passes of the NN, and code
                  using values of Q to choose an action)</li>
              </ul>
            </p>
            <p class="text-article">
              Here we consider the generality of a control system as its ability
              to do things beyond predefined behavior. On the low end, we have
              control systems like reflexes, which are essentially fixed. On the
              other end, we have systems like the logical consciousness, which
              has allowed humans to adapt to different cultures and ways of life
              throughout history. To some degree, generality of a control system
              places a lower bound on the ability to limit what information
              is provided to the control system; pre-determining that a control
              system doesn't need to be provided with certain information is to
              some degree pre-determining what the control system can and can't
              do.
            </p>
            <p class="text-article">
              From another perspective, distributing control but retaining
              generality imposes limitations on the latency of behavioral
              changes. For example, one could look at ant colonies as
              distributed systems where certain genes of each ant serves as
              behavioral programming.  Without changing any genes, the total
              system is certainly capable of adapting to different environments
              (i.e. it is far more general than something like reflexes) but its
              goals and overall functioning are still quite specific. The
              generality of genetics (minus selection pressures) allows for a
              much wider range of possible system behaviors, but because the
              genes are distributed amongst each node in the system, this wider
              system behavior can only change relatively slowly. Bypassing the
              slowness of evolution through reproduction, we can pretend these
              ants perform rapid,
              local <a href="https://en.wikipedia.org/wiki/Horizontal_gene_transfer">horizontal
              gene transfer</a>,
              like <a href="https://en.wikipedia.org/wiki/Bacterial_conjugation">conjugation</a>
              of <a href="https://en.wikipedia.org/wiki/Plasmid">plasmids</a>
              between bacteria. Even so, we still run into asymptotic problems:
              the larger the colony, the longer this system-wide evolution
              takes. Although here the best-case parallel control protocol's
              logarithmic
              asymptotic <a href="https://en.wikipedia.org/wiki/Time_complexity">time
              complexity</a> is not bad for a fairly wide range of problem
              sizes, distributing control
              imposes <a href="https://en.wikipedia.org/wiki/Bisection_bandwidth">bisection
              bandwidth</a> demands on the communication system, whereas
              centralizing control allows for high-bandwidth but
              low-bisection-bandwidth networks like fat tree networks. A similar
              example exists in the world of computer networks,
              where <a href="https://en.wikipedia.org/wiki/Router_(computing)">routers</a>
              help
              pass <a href="https://en.wikipedia.org/wiki/Network_packet">packets</a>
              from source to destination. One can view
              the <a href="https://en.wikipedia.org/wiki/Internet">Internet</a>
              roughly as a sphere, where end nodes like laptops, phones,
              and <a href="https://en.wikipedia.org/wiki/Server_(computing)">servers</a>
              exist on the surface, and routers exist inside the sphere to form
              paths between points on the surface. In order for routers to
              determine what neighbor to send a packet to (assuming none of its
              neighbors are the actual destination), routers communicate routing
              information
              using <a href="https://en.wikipedia.org/wiki/Routing_protocol">routing
              protocols</a>
              like <a href="https://en.wikipedia.org/wiki/Border_Gateway_Protocol">iBGP
              and eBGP</a> (for intra- and
              inter- <a href="https://en.wikipedia.org/wiki/Autonomous_system_(Internet)">autonomous
              system</a> communication, respectively). Thus, routers form a
              distributed control system for information passing on the
              Internet. More
              recently, <a href="https://en.wikipedia.org/wiki/Software-defined_networking">software-defined
              networking</a> has gained popularity for routing control within a
              single network because it centralizes control, allowing for fast,
              network-wide changes in behavior, like redirecting traffic to a
              different server if one server goes down, or blocking traffic from
              a malicious actor. The Internet as a whole still employs
              distributed control because of goals like resilience to failure
              (including something like the destruction of a geographical region
              through natural disaster or military attack,
              although <a href="https://en.wikipedia.org/wiki/Submarine_communications_cable">submarine
              communications networks</a> are a geopolitically significant
              exception) and an explicit desire for the Internet to not be
              controlled by any one entity. However, the efficiency and clarity
              conferred by centralizing control of networks is hard to compete
              with when ensuring distributed control is no longer an explicit
              goal.
            </p>
            <p class="text-article">
              It's worth pointing out that "centralized" control does not
              necessarily mean a single, exclusive control. Indeed, resilience
              generally
              demands <a href="https://en.wikipedia.org/wiki/Redundancy_(engineering)">redundancy</a>
              to avoid having
              a <a href="https://en.wikipedia.org/wiki/Single_point_of_failure">single
              point of failure</a>. Even so, completely centralized control
              (i.e. a single leader) can still be built on top of distributed
              systems. Consider
              the <a href="https://en.wikipedia.org/wiki/Raft_(algorithm)">raft</a>
              consensus algorithm, which employs distributed leader election.
            </p>
            <p class="text-article">
              Now we reiterate that all of the above analysis is
              asymptotic. That is, we consider how different factors come into
              play as the scale of the system grows rather than considering the
              system to be of a fixed
              size. <a href="https://en.wikipedia.org/wiki/Direct_democracy">Direct
              democracy</a> is a great example of this, where matters of
              efficiency in decision-making are negligible at the scale of 10
              people, but are absolutely crushing at the scale of 10 million
              people. Different democratic systems are built by applying
              democratic principles in different ways to attempt to work around
              issues of scale. Directly voting for a leader or on non-immediate
              policy works (more or less) because efficiency is not particularly
              important for those decisions.
              Similarly, <a href="https://en.wikipedia.org/wiki/Representative_democracy">representative
              democracy</a> essentially serves as a hierarchical application of
              direct democracy, trying to constrain scale at each level in the
              hierarchy and handling more urgent decision-making in the higher
              (and smaller) levels. Of course, political systems deal with
              design goals beyond the three points above, such as
              representation, fairness, and resilience to greed, corruption,
              sabotage, uneducated voters, and misinformation, to name a few.
            </p>
            <p class="text-article">
              Returning to our original three design factors, we claim that in
              trying to attain low latency, high amount &amp; diversity of
              integrated information, and high generality, a metasystem
              transition will experience an asymptotic pressure to centralize
              control, creating an explicit metasystem. The important caveat
              here is that within larger-scale centralization can exist
              smaller-scale decentralization. Different macroscopic regions of
              the neocortex could potentially approach consensus in a
              decentralized manner because of the relatively small number of
              regions and the close spatial proximity (a great benefit to
              latency and efficiently implementing high-bisection-bandwidth). In
              some
              modern <a href="https://en.wikipedia.org/wiki/Data_center">datacenter</a>
              networks with high bisection bandwidth, the pressure to centralize
              control based on the efficiency of fat tree networks is not very
              strong, but latency is still relevant. The higher the latency
              between two machines, the more pressure the control protocol would
              experience for direct or indirect interaction between these two
              machines to occur less frequently; low-latency decision making can
              only occur by the control not requiring much (if any)
              communication between those two machines in order to make the
              decision.
              High <a href="https://mathworld.wolfram.com/GraphDiameter.html">graph
              diameter</a>&#8213;considering
              some <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)">metric</a>
              based on latency&#8213;regardless imposes lower bounds on the
              latency of system-wide behavioral changes if general control is
              distributed amongst the entire graph;
              <a href="https://en.wikipedia.org/wiki/Eventual_consistency">eventual
              consistency</a> semantics serve as an imperfect middle-ground
              here, but is a leaky abstraction that the control protocol must
              design around, <em>especially</em> when desiring disparate
              information to be highly-integrated.
            </p>
            <p class="text-article">
              Considering the subconscious
              and unconscious systems in the context of two interrelated
              metacognitive loops within a single brain, having the neocortex
              surround lower brain structures is a particularly effective
              arrangement at the scale of two systems&#8213;enabling spatial
              proximity and relatively high surface area for
              information-sharing&#8213;which suggests there would not be a
              strong design pressure to centralize control rather than
              distributing between the two. On the other hand, the evolutionary
              pressure to keep certain functionality unconscious is an
              additional factor for further centralization in those specific
              cases.
            </p>

            <br>
            <h4 class="text-center" style="text-transform: none">Consciousness as a Metasystem Transition</h4>
            <p class="text-article">
              For the sake of discussion, let's look at the subconscious system
              as a collection of many small processes that can run in
              parallel. Given the analysis of the previous sections, it would not be at all
              surprising that related processes would be located near each
              other, both for communication between processes and for enabling related
              input to be transported to a single macroscopic
              region. We might see the resulting system as a
              hierarchy/heterarchy of control between specialized regions, not
              entirely unlike the example above of representative democracy. This alone
              leaves much room for varying levels of centralized and distributed
              control at various levels and locations. In the context of the
              logical consciousness, metacognitive control could be distributed
              amongst different macroscopic regions, behaving similarly to the
              distributed consensus proposed for certain logical-somatic
              interactions. However, the speed of metacognitive analysis places
              a strict upper bound on the frequency of the metacognitive loop's
              iterations; slow metacognitive analysis necessitates a slow "speed
              of conscious thought". Given the strong pressure for low-latency,
              as well as information requirements that are potentially very
              high, there would be a design pressure for this control to be
              represented as an explicit metasystem, which we postulate is one
              of the purposes of the prefrontal cortex. In a sense, the
              development of a single system (here the neocortex) into multiple
              specialized subsystems which interact with each other constitutes
              a metasystem transition. The development of the prefrontal cortex
              might be seen from this perspective as a progression of the
              metasystem transition, making part of the metasystem
              explicit. While a similar analysis might be made for the somatic
              consciousness, including more diverse brain structures, the
              remarkable <a href="https://en.wikipedia.org/wiki/Cortical_minicolumn">structural
              uniformity</a> of the neocortex makes it a distinctive example of a
              metasystem transition in this context.
            </p>
            <p class="text-article">
              Within the metasystem transition from a raw computational system
              to a computational system which controls itself, we might describe
              consciousness-as-control as the process by
              which the metasystem enacts control. A metacognitive loop is a
              relatively straightforward and explicit example of a means of
              implementing consciousness-as-control. If we are to consider
              consciousness-as-control as characterizing the metasystem, then we might also
              consider the distributed consensus amongst subconscious logical
              and somatic thoughts as an implicit form of
              consciousness-as-control. In particular, since this is an
              implicit control over conscious processes themselves (here,
              explicit conscious processes in the form of metacognitive loops),
              we might call this distributed consensus an implicit
              meta-conscious process. We will briefly explore this line of thought
              further, but we first try to further qualify consciousness.
            </p>
            <p class="text-article">
              One important characteristic of a conscious process, particularly
              in the context of producing conscious experience, seems to be the
              amount and degree to which information is integrated. That is, we
              consider conscious experience to partially be the product of
              integrating disparate information rather than having different
              subsystems process different information in hermetically sealed
              environments. Unlike
              the <a href="https://en.wikipedia.org/wiki/Integrated_information_theory">integrated
              information theory</a>, which takes consciousness as an axiom and
              therefore quantifies it on the basis of system properties which
              are only axiomatically tied to consciousness itself (a bit too
              reminiscent of the arbitrariness characterized by quantifying
              intelligence
              using <a href="https://en.wikipedia.org/wiki/Intelligence_quotient">IQ</a>
              for my tastes), here we constrain this concept specifically to the
              context of consciousness as qualified in the above
              sections. Nevertheless, consciousness seems to me inextricable
              from intelligence&#8213;after all, they are both simply different
              lenses for analyzing the same artificial and biological
              brains&#8213;and so I would posit that "degree of consciousness"
              cannot be generally quantified without quantifying intelligence;
              something like
              the <a href="https://en.wikipedia.org/wiki/Glasgow_Coma_Scale">Glasgow
              Coma Scale</a> relies on limited generality so that the scale can
              be useful in the contexts where it is employed. While we must
              defer this to a future article, I would argue that general
              intelligence cannot easily be sanely quantified as a single
              numeric vector (mapping
              some infinite-dimensional <a href="https://en.wikipedia.org/wiki/Function_space">function
              space</a> to an
              infinite-dimensional <a href="https://en.wikipedia.org/wiki/Real_number">real</a>
              space not falling under the "sane" category here), and that
              asymptotic analytic techniques used for time and space complexity
              will not be as useful in the context of "intelligence
              complexity". Certainly we would expect quantification of human
              intelligence to be a vastly simpler undertaking than the
              quantification of general intelligence.
            </p>
            <p class="text-article">
              Regardless, we can still reason about relative degrees of
              consciousness without concrete numbers and equations. On the far
              lower-end of information integration, a set of nodes whose only
              inter-communication
              is <a href="https://en.wikipedia.org/wiki/Heartbeat_(computing)">heartbeats</a>
              is not likely creating much of an integrated conscious experience,
              even if each of the nodes on their own are. Relating back to
              meta-consciousness, societies of humans such as countries,
              companies, and families might be seen as forms of
              meta-consciousness, but the degree of information integration is
              still fairly low (compare to the interactions between the somatic
              and logical consciousnesses). Nevertheless, the ability of these
              meta-consciousnesses in exerting some control on the component
              consciousnesses is not negligible, even if they themselves lack
              the building blocks for deep conscious experience. It is
              interesting to consider something like an AI which is composed of
              hundreds or thousands of deeply integrated logical
              consciousnesses. While the bisection bandwidth and compute power
              of future datacenters doesn't preclude such a possibility, what
              would such a meta-consciousness look like? If the scale produces a
              design pressure to centralize control, does the explicit
              meta-conscious process behave like a regular conscious process, or
              would it be different in nature?  Since a meta-consciousness
              integrates multiple consciousnesses, does the resulting
              integration of information collapse the levels of consciousness
              from the perspective of conscious experience, or does the
              experience continue evolving into something else?
            </p>
            <p class="text-article">
              Considering the consciousness of different animals, we begin by
              pointing out that all mammals have a neocortex of some
              sophistication, and
              the <a href="https://en.wikipedia.org/wiki/Pallium_(neuroanatomy)">pallium</a>
              of other vertebrates could serve
              as <a href="https://en.wikipedia.org/wiki/Convergent_evolution">analogous
              structures</a> with respect to consciousness. If nothing
              else, evolution shows us that there is very rarely one way of
              achieving a goal through system architecture. I tend to believe
              that my favorite genus of
              spider, <a href="https://en.wikipedia.org/wiki/Portia_(spider)">Portia</a>,
              is capable of far deeper conscious experience than most humans would
              reserve for
              <a href="https://en.wikipedia.org/wiki/Arthropod">Arthropoda</a>. It
              seems well-accepted in some scientific and pseudoscientific
              circles to consider a belief of animal subjective experience
              as <a href="https://en.wikipedia.org/wiki/Anthropomorphism">anthropomorphism</a>. However,
              there is nothing inherently scientific about taking such a
              position; this fundamentally boils down to how one constructs
              their <a href="https://en.wikipedia.org/wiki/Exclusion_of_the_null_hypothesis">null
              hypothesis</a>. I would argue that
              the <a href="https://en.wikipedia.org/wiki/Burden_of_proof_(philosophy)">burden
              of proof</a> rests on those claiming that an animal
              does <em>not</em> have conscious experience, not only because
              there exist
              many <a href="https://en.wikipedia.org/wiki/Clade">clades</a>
              which we share with various animals, but also because the
              development of conscious processes <em>specifically</em> which
              would allow for conscious experience seems to be highly beneficial
              evolutionarily. We might use brain structure and development as a
              basis for claiming relative degrees of conscious experience
              between species, but to look at species with complex nervous
              systems and claim a lack of conscious experience seems to be
              an <a href="https://en.wikipedia.org/wiki/Sagan_standard">extraordinary
              claim</a>.
            </p>
            <p class="text-article">
              On the other hand, artificial intelligence, which is not subject
              to the kinds of evolutionary pressures that animals are&#8213;even
              when produced
              through <a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">evolutionary
              algorithms</a>&#8213;is not as clear-cut. I would prefer to ground
              my trust in an AI's claims of subjective experience by
              understanding the workings of the system (to some degree, which
              abstractions allow us to do even when we cannot understand at once
              every facet of a complex system) partly according to the lens
              described by this article. However, the path that AI is on has
              already led to systems that can only be very superficially
              analyzed and guided by humans, and the developments of the past
              decade in particular have rested squarely on more and more
              absolute relinquishing of such human control. I believe that for
              AGI to truly be beneficial to humanity, it must be able to
              understand and relate to human conscious experience (i.e. must be
              capable of empathy, as we've defined it above), and I am dubious
              of the implicit assumption of the deep learning community that we
              will blindly stumble our way into an AI that not only has human or
              superhuman intelligence, but that will be a net benefit to
              humanity. While there are many in the field who might not agree
              with that implicit assumption either, the actions of the field
              unfortunately speak far louder than the unexercised beliefs of
              individuals within the field. The story of humanity seems to be
              one of reaction, where actions are quickly taken without fully
              analyzing&#8213;or even being concerned with&#8213;consequences,
              and the mitigation of inevitable fallout left to faceless masses
              of future sufferers. Such an approach will not work this time, but
              humans have not changed enough to act differently, even as climate
              change displays the folly of humanity front-and-center. Because of
              this, the choice is not between AI or no AI, but rather between
              the outcome of AI as a net benefit or detriment to humanity. If we
              must get things right the first time around, then AI is not just a
              race between companies and countries, but a race between fates of
              humanity.
            </p>
            <br>

            <h3 class="section-heading section-line text-center"><br>References</h3>
            <p class="text-article text-no-bottom">
              <a href="#rr1" id="r1">[1]</a> Block, N. (1995) <a href="http://cogprints.org/231/1/199712004.html" target="_blank">On a confusion about a function of consciousness.</a> Behavioral and brain sciences, 18(2), 227-247.
            </p>

            <p class="text-article text-no-bottom">
              <a href="#rr2" id="r2">[2]</a> Hurlburt, R. (2011). Investigating Pristine Inner Experience: Moments of Truth. Cambridge: Cambridge University Press. doi:10.1017/CBO9780511842627
            </p>

            <p class="text-article text-no-bottom">
              <a href="#rr3" id="r3">[3]</a> Holm, S., Eilertsen, T., & Price, M. C. (2015). <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4566903" target="_blank">How uncommon is tickertaping? Prevalence and characteristics of seeing the words you hear.</a> Cognitive neuroscience, 6(2-3), 89–99. https://doi.org/10.1080/17588928.2015.1048209
            </p>

            <p class="text-article text-no-bottom">
              <a href="#rr4" id="r4">[4]</a> Rayner, K., Pollatsek, A., Ashby, J., & Clifton Jr, C. (2012). <a href="http://www.psy.gla.ac.uk/~simon/S00606.pdf" target="_blank">Psychology of reading</a>. Psychology Press.
            </p>

            <p class="text-article text-no-bottom">
              <a href="#rr5" id="r5">[5]</a> Smith, B. C. (1984, January). <a href="https://www.ics.uci.edu/~jajones/INF102-S18/readings/17_Smith84.pdf" target="_blank">Reflection and semantics in Lisp</a>. In Proceedings of the 11th ACM SIGACT-SIGPLAN symposium on Principles of programming languages (pp. 23-35).
            </p>

            <p class="text-article text-no-bottom">
              <a href="#rr6" id="r6">[6]</a> Arakaki, Y., Kawai-Toyooka, H., Hamamura, Y., Higashiyama, T., Noga, A., Hirono, M., Olson, B. J., & Nozaki, H. (2013). <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3859500/">The simplest integrated multicellular organism unveiled</a>. PloS one, 8(12), e81641. https://doi.org/10.1371/journal.pone.0081641
            </p>

          </div>
          <div class="col-lg-2" id="leftRef">
          </div>
        </div>

        <br><br>
        <center>
          <p class="text-article">
            Questions, concerns, or inaccuracies can be reported as a GitHub issue
            <a href="https://github.com/akhouderchah/akhouderchah.github.io/issues/new?assignees=akhouderchah&labels=article%3A+consciousness&template=consciousness-article-issue.md&title=%5Bconsciousness%5D+ISSUE+DESCRIPTION">here</a>.
          </p>
        </center>

      </div>
    </section>

    <footer>
      <div class="container">
        <div class="row">
          <div class="col-md-4">
            <span class="copyright">Copyright &copy; Alex Khouderchah 2020</span>
          </div>
          <div class="col-md-8">
          </div>
        </div>
      </div>
    </footer>

    <!-- jQuery -->
    <script src="../../vendor/jquery/jquery.min.js"></script>
    <script src="../../js/jquery.easing.1.3.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="../../vendor/bootstrap/js/bootstrap.min.js"></script>

    <!-- Article JS -->
    <!-- TODO - make min -->
    <script src="../../js/article.js"></script>

  </body>

</html>
